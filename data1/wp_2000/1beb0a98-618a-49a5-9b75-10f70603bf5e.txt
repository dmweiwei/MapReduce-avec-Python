Le spamdexing ou référencement abusif, est un ensemble de techniques consistant à tromper les moteurs de recherche sur la qualité d'une page ou d'un site afin d'obtenir, pour un mot-clef donné, un bon classement dans les résultats des moteurs (de préférence dans les tout premiers résultats, car les utilisateurs vont rarement au-delà de la première page qui, pour les principaux moteurs, ne comprend par défaut que dix adresses).
Elle est parfois sévèrement punie par les moteurs, même s'il n'y a pas de code de conduite précis pour les référenceurs (il est parfois difficile de distinguer le référencement abusif de la SEO de l'optimisation "honnête"). Les techniques habituelles de référencement abusif consistent par exemple à truffer une page satellite de listes de mots-clés (pour attirer les utilisateurs de moteurs qui font une recherche sur ces mots), ou à créer des dizaines de sites qui pointent les uns vers les autres (link farms ou pépinières de liens) pour améliorer leur classement dans les moteurs qui jugent la qualité d'une page en fonction du nombre de liens pointant vers elle.

Terminologie
Le mot "spamdexing" est un néologisme anglophone composé du substantif "spam" et du suffixe "dexing" pris sur le terme "indexing" signifiant référencement. Au Canada, l'Office québécois de la langue française propose comme traduction de "spamdexing" "référencement abusif". En France, on n'a pas encore proposé de traduction officielle.

Fonctionnement
En principe, les moteurs de recherche classent les résultats selon la qualité des pages et leur pertinence par rapport à la requête ; mais les moteurs actuels (s'opposant ainsi aux annuaires, produits par des humains, qui refusent les sites de qualité insuffisante) tentent d'estimer la qualité et la pertinence des pages par des procédés automatiques, dont les principes sont connus, dans leurs grandes lignes, par les spammeurs et les optimiseurs de sites : une page est supposée de bonne qualité si un grand nombre de liens externes pointent vers elle (quand un concepteur de page web place un lien vers une page, il est ainsi censé "voter" pour cette page) ; il est facile de créer plusieurs sites qui pointent vers le site qu'on veut promouvoir (ou bien d'échanger des liens avec des sites amis, gérés par d'autres personnes. C'est le tissage de liens ;; une page est supposée pertinente, en réponse à une requête donnée, si elle contient beaucoup de mots présents dans la requête ;; les différents mots de la page obtiennent un poids plus important selon leur emplacement (par exemple, si l'expression "vente de voitures" figure dans le titre, la page est très probablement consacrée à ce sujet) ;; les moteurs tiennent également compte des mots présents dans l'adresse de la page (ce qui explique qu'on trouve parfois des URL longues, avec des répétitions de mots, comme www.exemple.com-voyages-pas-chers-voyage-en-chine-voyage-en-chine.html).
Les techniques de référencement évoluent dans le temps et s'adaptent aux moteurs. Une nouvelle technique voit le jour : la "saturation par des intégrations multiples". Le principe est le suivant : le titulaire du site à promouvoir propose son contenu à une série de partenaires qui ont un nom de domaine avec classement élevé par PageRank et un nombre de pages élevé, ce qui facilitera leur ascension dans les résultats. Exemple : www.site-du-spamdexeur.com propose le contenu. Puis, on retrouve le même contenu sur http:--mot-clé.partenaire.com, http:--mot-clé.partenaire2.com, etc.. En résulte une saturation de la page de résultats des moteurs de recherche. On peut ainsi réussir à obtenir 80 % des résultats de recherche affichés en première page par les moteurs de recherche. Comme la plupart des clics se font sur la première page de résultats d'une requête, ils s'assurent ainsi un maximum de visibilité et évincent leurs concurrents.

Le référencement éthique
Par opposition aux techniques de référencement dites abusives, certaines personnes avancent l'idée d'un référencement "éthique" censé reposer sur un code déontologique. Divers sites, ou associations de référenceurs, se sont avancés à proposer leur vision d'un code déontologique en matière de marketing des moteurs de recherche. Ces préceptes n'ont aucune force de loi, varient d'une appréciation individuelle à l'autre, et n'engagent que ceux qui veulent bien se reconnaître dans de tels modèles "éthiques". Ces mêmes codes d'éthique sont rédigés par intimidation des moteurs de recherche.
Les moteurs de recherche ont un succès inégal face au référencement abusif. Certaines pages peuvent rester bien positionnées alors que d'autres sont lourdement sanctionnés. Ces actions peuvent être difficiles à interpréter, car les moteurs ne donnent pas toujours d'information sur les raisons de son agissement. Cette variabilité dans le traitement tend à confirmer que les algorithmes d'indexation mis en oeuvre sont complexes et difficiles à interpréter.
Un moteur comme Google applique des pénalités manuelles à certains sites mais tend à privilégier les solutions algorithmiques.

Dissimulation du spam
Pour ne pas donner des soupçons à l'utilisateur qui verrait sur son écran une longue liste de mots, les nombreux termes placés dans une page pour "piéger" les moteurs sont souvent camouflés par différents procédés : relégation de ces listes de mots en bas de page ;; écriture en caractères minuscules ;; mots placés dans une section "noframes", "noscript" ou "display:none" (généralement non affichée par le navigateur, mais lue par les robots des moteurs) ;; caractères de même couleur que le fond de la page (ce qui rend le texte invisible) ;; moteurs ou annuaires affichant de longues listes de "dernières recherches" ou de "recherches populaires" ;; pages dynamiques - par exemple celles de moteurs de recherche - déguisées en pages statiques, avec des adresses telles que example.com-trouver-requete.php: une telle adresse ressemble à celle d'un fichier statique qui s'appellerait trouver-requete.php, et qui serait situé sur le serveur du moteur, alors qu'il s'agit en fait d'une page dynamique (la sortie d'un script PHP, affichant les résultats de recherche) créée au moment de la requête : le fait de "déguiser" ainsi l'URL permet de faciliter son indexation si on suppose que les pages dynamiques peuvent ne pas être indexées par les moteurs, ou obtenir un classement inférieur à celui des pages statiques. En général, les pages de résultats des principaux moteurs possèdent des adresses telles que example.com-search.cgirequete, où le contenu de la requête n'est pas déguisé en nom de fichier ; de plus, ces moteurs interdisent expressément l'indexation de ces pages au moyen d'un fichier robots.txt ;; Retrait des mots via un script (ex.: JavaScript) ;; Une page satellite, truffée de mots-clés, est lue par les robots des moteurs de recherche. Mais quand un humain la consulte, il est redirigé vers une autre page (et donc il ne voit pas la fausse page) ;; Le cloaking, occultage, consiste à présenter des résultats différents selon le logiciel utilisé pour afficher la page: une page anodine pour un navigateur web, une page optimisée, remplie de mots-clefs, réservée aux robots des moteurs ;; Les sociétés de SEO, sur leur page d'accueil, donnent des exemples de sites qu'elles ont optimisés, chacune de ces adresses étant placée derrière un mot décrivant le sujet du site en question; ce qui permet aux pages des optimiseurs de contenir des mots qui n'ont rien à voir avec leur activité (et donc de figurer parmi les résultats des recherches portant sur ces mots). Elles peuvent aussi mettre un lien vers leur propre site dans chaque page qu'elles modifient.; Le zurnisme est la création d'un néologisme pour obtenir une exclusivité de référencement. Le mot zurnisme est lui-même un zurnisme, qui a été créé en 2007 par un blog français. Aucun moteur n'est capable d'identifier l'intention cachée derrière la création d'un néologisme.

Listes noires
Les techniques frauduleuses (fermes de liens, cloaking et autres) sont surveillées, et parfois réprimandées, par les moteurs de recherche qui perfectionnent chaque jour leurs algorithmes afin d'éliminer les sites trichant pour leur positionnement.
Lorsque des fraudes sont constatées (par un robot, par un humain, ou par un concurrent), la page ou le site en question est placé sur une liste noire (désindexation du moteur de recherche, ce qui pénalise grandement le site) pour une durée de quelques mois, définitive ou jusqu'à ce que le site respecte les conditions d'indexation de l'outil.
Google a mis en place un "bac à sable" (sandbox) qui est un avertissement avant la mise sur liste noire, mais aussi un moyen d'empêcher les nouveaux sites de grimper trop rapidement sur certains mots-clés dans les résultats de recherche.
Google par ailleurs, propose une page en 93 langues pour maintenir la qualité de l'index avec un rapport de spam lié aux pratiques interdites citées ci-dessus, ce qui permettra d'entrer un jour dans le référencement éthique.

Voir aussi

Articles connexes: Bombardement Google; Splog; Pagejacking; Élément meta; Optimisation pour les moteurs de recherche. Portail de la sécurité informatique.