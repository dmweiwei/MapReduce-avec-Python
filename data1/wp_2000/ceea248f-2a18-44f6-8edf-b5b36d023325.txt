On appelle "révolution numérique" (ou plus rarement "révolution technologique" ou "révolution internet") le bouleversement profond des sociétés survenu globalement, dans les nations industrialisées (notamment Europe occidentale, Amérique du Nord, Japon) et provoqué par l'essor des techniques numériques, principalement l'informatique et Internet. Cette mutation se traduit par une mise en réseau planétaire des individus, de nouvelles formes de communication (courriels, réseaux sociaux) et une décentralisation dans la circulation des idées.

Éléments de définition
L'expression "révolution numérique" fait désormais partie des sciences humaines et l'abondante littérature reprenant cette expression peut donner l'impression d'une conception consensuelle du sujet. Pour autant, les analyses de l'historien François Jarrige démontrent l'émergence progressive d'un mouvement citoyen technocritique au fil du XIXe et du XXe siècle et le fait que l'association des mots "révolution" et "numérique" pose un certain nombre de questions d'ordre philosophique, dans un contexte qui dépasse donc largement l'avènement de l'informatique et qui porte sur le progrès technique en général.

Une expression répandue: Le mot "numérique" renvoie au processus de numérisation, qui consiste à reproduire techniquement les valeurs d'un phénomène physique non plus sur le mode analogique, comme on faisait avant l'avènement des technologies numériques, mais en convertissant toutes les informations qui le constituent en données chiffrables que des matériels informatiques (ordinateurs, smartphones, tablettes, etc.) peuvent ensuite traiter, ayant été conçus et fabriqués pour cela.; Le mot "révolution" renvoie aux multiples espoirs que fait naître cette mutation, principalement celui d'une réappropriation de l'espace public par les citoyens.
Comme la révolution industrielle, survenue deux siècles plus tôt, la révolution numérique est provoquée par un changement technique. Elle est directement associée à la naissance puis au développement de l'informatique, c'est-à-dire au fait que toute information (caractère d'imprimerie, son, forme, couleur, puis mot, texte, photographie, film, musique, etc.) peut être numérisée, c'est-à-dire s'exprimer par une combinaison de nombres (en l'occurrence des 0 et des 1) puis stockée, modifiée, éditée (sur des sites ou des blogs), et transmise (par mails, sur des forums, etc.) au moyen de toutes sortes d'appareils comme des ordinateurs, des tablettes, ou des smartphones. Ces premiers étant, depuis les années 1960, équipés de circuits intégrés, qui sont de taille réduite et peu consommateurs en énergie, ils permettent à des millions d'individus d'effectuer de façon de plus en plus automatique des tâches sans cesse plus nombreuses, complexes et dans des délais de plus en plus courts, au point qu'ils sont qualifiés comme dotés d'intelligence artificielle.
Sont distingués habituellement trois tournants décisifs: dans les années 1980, la généralisation de l'ordinateur personnel et la naissance d'internet ;; dans les années 1990, l'explosion du phénomène internet, surnommé "le réseau des réseaux" ;; dans les années 2000, l'apparition du smartphone, ordinateur tenant dans la main et pouvant être utilisé pratiquement partout sur le globe.
Ces innovations permettant aux échanges de s'opérer sous une forme électronique, les barrières géographiques et culturelles cessent d'être aussi contraignantes que par le passé. Cette mutation bouleverse l'ensemble des règles géopolitiques mondiales (mondialisation), l'économie planétaire (avènement de la Nouvelle économie) et, plus radicalement, la façon dont les individus perçoivent le monde, se comportent avec autrui et se considèrent eux-mêmes.
Le concept de révolution numérique est fondé sur l'idéal de progrès, qui a émergé en Europe à la fin du XVIIIe siècle avec les Lumières. Couverture de Éléments de la philo-sophie de Newton, mis à la portée de tout le monde, Voltaire, 1738
L'ambiguïté de l'expression
Le sociologue Jacques Ellul, à la fois spécialiste de la révolution et analyste de l'impact des techniques sur les mentalités, affirme que la révolution n'est plus qu'un mythe : "le mythe de la révolution dévale sur le monde moderne : (le) mot (est) galvaudé, utilisé pour tout et n'importe quoi.". De plus, l'usage du mot "technologie" relève, selon lui, du détournement langagier : "(...) ce mot "technologie", que l'usage abusif implante dans nos cerveaux, en imitant servilement l'usage américain qui en est le fondement. Le mot "technologie", quel qu'en soit l'emploi moderne des médias, veut dire "discours sur la technique.
Dans la mesure où l'économie mondiale est fortement influencée par les géants du web (Google, Facebook, Amazon, Microsoft, Twitter, LinkedIn, etc.), dont les chiffres d'affaires dépassent les PIB de nombreuses nations, l'expression "révolution numérique" prend un sens problématique : il est marqué par la prégnance considérable de l'économie sur la sphère politique et sur les modes de vie.
L'expression "révolution numérique" pose problème dans la mesure où elle réunit deux mots d'origines très différentes : le mot "numérique", qui renvoie à l'évolution de l'informatique, et le mot "révolution" qui est l'objet d'une multiplicité d'interprétations.
Le phénomène numérique peut être analysé à partir des caractéristiques des outils qu'il met en place ainsi qu'à partir de leur évolution : ils sont de plus en plus nombreux, se fondant de plus en plus à notre environnement, voire s'y substituant (cf. domotique) ;; ils sont de plus en plus petits, pouvant être introduits sous la peau ou dans nos corps (cf. miniaturisation et nanotechnologies) ;; ils sont de plus en plus intelligents, capables d'assumer plusieurs fonctions différentes (cf. intelligence artificielle) ;; ils sont de plus en plus autonomes, pouvant même communiquer entre eux (cf. internet des objets) ;; ils sont conçus pour fonctionner en réseau et permettre à leurs utilisateurs de surmonter aisément les contraintes physiques (cf. dématérialisation); ils confèrent artificiellement du pouvoir à leurs utilisateurs : filmer-enregistrer autrui à son insu ou le géolocaliser, se repérer dans n'importe quel espace, percevoir à la fois le réel et le virtuel, etc. (cf. géolocalisation, réalité augmentée).
Partant de ces caractéristiques, il est possible d'ébaucher une histoire de la "révolution numérique", un portrait l'assimilant au progrès. La première partie de cette page y est consacrée, un soin particulier étant pris à établir le lien unissant la "révolution industrielle" (XVIIe et XIXe siècles) et la "révolution numérique" (XXe et XXIe siècles), phénomènes fréquemment comparés.
Toutefois, le mot "révolution" étant très connoté, l'expression "révolution numérique" ne fait pas l'unanimité et oppose différentes sensibilités, essentiellement deux : d'un côté, majoritaires, les "technophiles", déclarés ou non, qui assimilent la mutation à un "progrès social" et pour qui l'usage de cette expression est pleinement justifié ; de l'autre, beaucoup moins nombreux et de posture "technophobe" ou simplement "critique", ceux qui considèrent que la prolifération et le perfectionnement des applications de l'informatique est un processus qui, comme la Révolution industrielle, met en jeu un rapport inconscient et prométhéen à la technique. Un processus qui, du coup, échappe au contrôle des hommes (en particulier à la démocratie) et qui, à terme, les expose autant, sinon plus, à des désagréments, des risques et des dangers qu'il ne leur procure des bienfaits. Sous cet angle, la "révolution numérique" prend le visage de quelque chose de négatif : l'aliénation.
Il est donc nécessaire d'établir une histoire de la réception de la "révolution numérique", c'est-à-dire non seulement d'analyser la façon dont le phénomène est vécu, mais également comment l'expression elle-même est interprétée.

Histoire
Les premiers ordinateurs étaient de simples machines à calculer : les informations qu'ils avaient à traiter étaient exclusivement des nombres. Comprendre l'histoire du numérique nécessite donc de saisir l'histoire du calcul.
Un boulier
Très tôt, les humains ont conçu et fabriqué des outils les aidant à calculer (abaque, boulier...). Mais c'est à partir du XVIIIe siècle qu'ils ne cessent de les perfectionner, quand s'amorce (en Angleterre puis en France) la Révolution industrielle. Alors que la société s'était bâtie sur une économie à dominante agraire et artisanale, elle s'urbanise de façon croissante, devenant de plus en plus commerciale et industrielle. Dans le but de rendre la production toujours plus efficace, les machines sont conçues et fabriquées à un rythme exponentielle. Au fur et à mesure que la société se mécanise, émerge l'idée selon laquelle la machine ne doit pas seulement aider les hommes, mais aussi, autant que possible, les remplacer. Le goût pour les automates, qui se développe à cette époque, traduit un désir plus ou moins conscient : celui que toutes les étapes d'un processus de production (conception, fabrication, maintenance, commercialisation, etc.) soient prises en charge par une "machinerie intelligente", c'est-à-dire habilitée à traiter un maximum d'information automatiquement et à la place de l'homme. Il est donc d'usage de considérer "la révolution numérique" comme le prolongement logique de la révolution industrielle.

Genèse

XVIIe siècle
La pascaline, toute première machine à calculer (1642)
L'esprit des télécommunications s'institutionnalise. En 1603, en France, le roi Henri IV fait créer un corps de courriers (estafettes) chargé de transporter les correspondances aussi bien administratives que privées : c'est la naissance officielle de "la poste", administration détenant le monopole de ce service. En 1612, est mis en place un service de diligences transportant à la fois du courrier, des paquets et des voyageurs.
Dans la deuxième moitié du siècle, deux philosophes, l'Allemand Gottfried Leibniz et l'Anglais Thomas Hobbes, émettent l'hypothèse que la pensée peut se formuler de façon systématique par le biais d'un langage mathématique. Le premier imagine un langage assimilant l'argumentation à un calcul, afin qu'"il n'y ait pas plus de besoin de se disputer entre deux philosophes qu'entre deux comptables". Selon Hobbes, "la raison n'est rien d'autre que le fait de calculer. Mais c'est un autre philosophe, le Français Blaise Pascal, qui entreprend de concrétiser ces principes en inventant la pascaline dès 1642 : la toute première machine à calculer dont le fonctionnement permet de traiter un algorithme.

XVIIIe siècle

Les débuts de l'automation
Durant la première moitié du siècle émergent des inventions qui relèvent de l'automation et qui annoncent ce qui deviendra plus tard l'informatique. En 1728, dans le but d'automatiser le fonctionnement des métiers à tisser, le français Jean-Baptiste Falcon invente le système de la carte perforée : morceau de papier rigide contenant des informations représentées par la présence ou l'absence de trou dans une position donnée. En 1735, pour les plaisirs de la Cour, Jacques Vaucanson construit son premier automate, le flûteur automate. Puis, en 1744, il en construit un autre plus sophistiqué et qui fait forte impression sur le public : le canard digérateur. Nommé inspecteur général des manufactures de soie et chargé de réorganiser cette industrie, il perfectionne le métier à tisser de Falcon en l'automatisant par hydraulique, la commande étant assurée par des cylindres analogues à ceux de ses automates.
La "révolution numérique" peut être perçue comme l'équivalent actuel de la "révolution industrielle", l'image de la machine servant de lien. Machine à vapeur, James Watt, (1769)
Révolution industrielle et Lumières : capitalisation du savoir-faire technique et de la connaissance
Durant la seconde moitié du siècle, en Grande-Bretagne, la machine à vapeur, mise au point par l'ingénieur écossais James Watt, et le réseau ferré transforment peu à peu les structures économiques et sociales du pays. Lewis Mumford voit dans la révolution industrielle la préfiguration de la "révolution numérique".
En France, les principaux acteurs de cette mutation sont issus de la bourgeoisie, une nouvelle classe sociale qui "détrône" l'aristocratie. Pleinement conscients et désireux de fonder une civilisation moderne, "éclairée", ils consignent par écrit l'ensemble de toutes les innovations scientifiques et techniques. Éditée de 1751 à 1772, L'Encyclopédie de Diderot et d'Alembert (ou "Dictionnaire raisonné des sciences, des arts et des métiers") promeut l'universalisme, lequel préfigure les notions de réseau et de village global aujourd'hui associées à l'idée de "révolution numérique".

XIXe siècle

Révolution industrielle et machinisme
La Grande-Bretagne affirme sa suprématie sur le reste du monde : 500 pompes à vapeur y fonctionnent en 1800, la première locomotive y circulant en 1803. Dans le premier quart du siècle, l'électricité reste une curiosité de laboratoire (pile inventée en 1801) en regard du développement de l'énergie thermique. À partir de 1835, la fièvre du rail s'empare de l'Europe. Le réseau ferroviaire peut être considéré comme une préfiguration du réseau internet. De la science au scientisme.
Ces mutations engendrent de tout nouveaux rapports entre la science et la technique : le scientifique cesse d'être un amateur et devient un professionnel formé par des études supérieures, accédant au statut d'ingénieur. L'industrie et la recherche se stimulent mutuellement, la première devenant l'application de la seconde, dynamique qui s'accentuera plus tard avec la "révolution numérique".
C'est dans ce contexte de perpétuelle innovation technique qu'émerge peu à peu une nouvelle vision du monde, le scientisme : non seulement la science supplante la religion dans sa vocation d'interpréter l'univers, mais certains estiment qu'elle doit s'arroger celle d' "organiser scientifiquement l'humanité" (la formule est du philosophe Ernest Renan). En France, les saint-simoniens considèrent que l'industrie doit prendre le pas dans la société et invitent les industriels à constituer un parti afin de prendre le pouvoir.
La machine : le moyen d'accroître la productivité Ici, une machine à rouler les cigarettes (1880)
Économisme et productivisme
Les rapports à l'économie sont également bouleversés, car le progrès technique contraint les industriels d'innover pour améliorer les taux de profit en abaissant les prix de revient. Par suite, l'économie devient de plus en plus productiviste et détermine le monde des idées, comme le démontre l'économiste allemand Karl Marx dans son étude sur les rapports entre superstructures et infrastructures. Est-il besoin d'une grande perspicacité pour comprendre que les idées, les conceptions et les notions des hommes, en un mot leur conscience change avec tout changement survenu dans leurs conditions de vie, leurs relations sociales, leur existence sociale Que démontre l'histoire des idées, si ce n'est que la production intellectuelle se transforme avec la production matérielle
Préfiguration de l'informatique
Une partie de la machine analytique de Babbage
Comme au siècle précédent, les signes annonciateurs de l'informatique sont encore très limités : En 1801, Joseph Marie Jacquard, avec son métier à tisser à cartes perforées, fait émerger le concept de programmation.; En 1834, le mathématicien anglais Charles Babbage, associant les inventions de Pascal et de Jacquard, conçoit la machine analytique (véritable ancêtre de l'ordinateur) alimentée par l'énergie à vapeur.; Dans les années 1840 et 1850, Ada Lovelace et George Boole développent des théories permettant non seulement de traiter des opérations mathématiques de manière automatique, mais également de traduire des concepts en équations.
En revanche, le siècle est marqué par des inventions décisives dans le domaine des télécommunications et qui, indirectement, précipiteront l'avènement de l'informatique. En 1844, Samuel Morse effectue la première démonstration publique du télégraphe en envoyant un message sur une distance de 60 km, entre Philadelphie et Washington. En 1858, le premier câble transatlantique est tiré entre les États-Unis et l'Europe pour interconnecter les systèmes de communication des deux continents. En 1876, l'Américain Graham Bell invente le téléphone et fonde la compagnie Bell Telephone Company. Par ailleurs, l'énergie électrique est de mieux en mieux maîtrisée. En 1879, l'américain Thomas Edison invente l'ampoule à incandescence et en 1892, l'Allemand Karl Ferdinand Braun invente le tube cathodique qui servira aux premiers écrans de télévision puis d'ordinateurs.

Première moitié du XXe siècle
Expérience de radio en 1918 à l'Université de New York
La technique investit l'environnement et pénètre les foyers
Durant les cinquante premières années du siècle, un grand nombre d'inventions voient le jour et sont aussitôt mises en application par l'industrie. Toutes contribueront plus tard à la "révolution numérique". Retenons principalement trois d'entre elles : Dans les années 1900, l'électricité investit l'industrie, l'éclairage public, le chemin de fer puis les foyers ;; En 1906, aux États-Unis, la voix est pour la première fois retransmise par les ondes radio et en1920, les premiers programmes quotidiens de radiodiffusion débutent en Angleterre, aux États-Unis et en URSS ;; en 1926, à Londres, l'Écossais John Logie Baird effectue la première retransmission télévisée publique en direct. En 1932, est retransmis en France un programme d'une heure par semaine : Paris Télévision. Une centaine de postes le reçoivent.

De l'électronique à l'informatique
En parallèle, les travaux préparant l'avènement de l'informatique se poursuivent. Dans les années 1930, Fredrik Bull crée en Suisse la première entreprise développant et commercialisant des équipements mécanographiques en utilisant le principe des cartes perforées. L'Allemagne nazie s'intéresse de près à ce procédé. En 1941, à Berlin, l'ingénieur Konrad Zuse met au point le Z3, calculateur électromécanique, qui constitue la première machine programmable pleinement automatique. À Londres en 1944, Colossus est le premier calculateur fondé sur le système binaire.
L'ENIAC, premier ordinateur mondial
Mais c'est aux États-Unis, plus précisément en Californie, à quelques kilomètres de San Francisco, très exactement à Palo Alto, que s'amorce véritablement la "révolution numérique". C'est là qu'en 1939, William Hewlett et David Packard y ont fondé dans un simple garage l'entreprise qui deviendra plus tard une multinationale. Cette vallée, qui sera baptisée Silicon Valley en 1971, constitue la première technopole mondiale. La fin de la Seconde Guerre mondiale marque le début d'une hégémonie des États-Unis en matière de progrès technique. En 1945, l'ingénieur Vannevar Bush imagine une machine à mémoriser stockant des microfilms. En 1946, à l'Université de Pennsylvanie, ENIAC devient le tout premier ordinateur mondial. Pesant 30 tonnes, occupant 167 m2, utilisant des tubes à vide et consommant 150 kilowatts, il effectue 5 000 additions par seconde. En 1948 est inventé le transistor, composant semi-conducteur de très petite taille et peu consommateur en énergie : il ouvre la voie à la miniaturisation des composants, ce qui fera par la suite de l'électronique l'un des principaux secteurs de l'économie.

Télévision, "société de consommation" et progrès
Alors que l'informatique est encore balbutiante, la télévision symbolise le progrès dans l'imaginaire collectif. Aux États-Unis, le nombre de récepteurs s'accroît de façon fulgurante : 30 000 en 1947, 157 000 en 1948, 876 000 en 1949, 3,9 millions en 1952. Témoin de l'American Way of Life, elle façonne les mentalités et crée la "société de consommation". Les spots publicitaires qui y sont diffusés accentuent d'autant le phénomène de l'achat compulsif, lequel se porte en priorité sur les objets techniques.

Du scientisme au technicisme
Dès cette période, les avancées techniques donnent naissance aux premières réflexions relatives à leur impact et leur signification dans les mentalités. De 1942 à 1953 se déroulent à New York les conférences Macy, qui réunissent des mathématiciens, logiciens, anthropologues, psychologues et économistes se donnant pour objectif d'édifier une science générale du fonctionnement de l'esprit. Parmi les participants, deux courants s'opposent : d'un côté le cercle "personnalité et culture", qui établit une réciprocité entre les sciences mathématiques et physiques et les sciences psychologiques (psychanalyse, psychologie du développement...) ; de l'autre, les "cybernéticiens", comme Norbert Wiener qui introduit en science la notion de feedback (rétroaction) qui aura des implications lourdes dans de nombreux domaines notamment en ingénierie, en informatique et en biologie. Wiener expose ses théories dans deux livres. Dans la seconde partie du second livre, "Cybernétique et société", il affirme que "de même qu'une révolution est en cours, permettant aux machines de remplacer les muscles de l'homme, une autre est en train de poindre qui leur permettra de se substituer à son cerveau". Les idées de Wiener contribueront à une adaptation au progrès technique.

Rejets et inquiétudes
La machine occupant une place croissante dans le monde ouvrier, celui-ci se mobilise pour ne pas en être esclave et lutte pour améliorer ses conditions de travail. En 1936, dans son film Les Temps modernes, le cinéaste anglo-américain Charles Chaplin décrit l'aliénation du travailleur par le machinisme. En 1949, l'écrivain anglais George Orwell dresse quant à lui un portrait très sombre de l'avenir. Son roman 1984 décrit un nouveau type de totalitarisme, caractérisé par la télésurveillance et le contrôle social.

Ère informatique

Années 1950
Alan Turing (ici âgé de 16 ans), l'un des initiateurs de l'intelligence artificielle
Les origines
Les innovations techniques successives ne sont pas sans inspirer les techniciens eux-mêmes. En 1950, dans son article "Computing Machinery and Intelligence", le mathématicien et informaticien anglais Alan Turing jette les bases de l'intelligence artificielle et fait "le pari que d'ici cinquante ans, il n'y aura plus moyen de distinguer les réponses données par un homme ou un ordinateur, et ce sur n'importe quel sujet". Mesurant l'ampleur de cette mutation et de son impact sur les mentalités, Jacques Ellul publie, en 1954, La Technique ou l'Enjeu du siècle qui constitue la toute première approche anthropologique du phénomène technicien. Selon lui, le développement de l'automation conduit la technique à se développer de façon autonome : celle-ci échappe à tout contrôle des hommes dès lors qu'ils s'obstinent à croire qu'elle n'est qu'un moyen neutre à leur service.
Au milieu de la décennie, naît aux États-Unis l'activité citizen-band (ou "CB", de l'anglais, "bande des citoyens"), première implication d'amateurs dans le domaine des télécommunications.
En 1957, les Soviétiques mettent sur orbite le premier satellite artificiel, Spoutnik 1. Cet événement ouvre une nouvelle étape dans l'ère des télécommunications : les satellites de télécommunication joueront plus tard un rôle indispensable dans la mise en place d'Internet.
Jack Kilby (au centre), inventeur du circuit intégré
Premières grandes avancées de l'informatique et de la télématique
L'année 1958 est marquée par deux événements majeurs : l'invention par Jack Kilby (de la société Texas Instruments) du circuit intégré, que l'on surnommera plus tard "puce électronique", et qui est un composant permettant à lui seul d'effectuer une ou plusieurs fonctions complexes sur un minuscule support en silicium ;; celle, par la société téléphonique Bell, du modem, qui est un périphérique permettant de transmettre des données binaires sur une ligne téléphonique.

De la spéculation scientifique au fantasme techniciste
En 1959, le physicien américain Richard Feynman anticipe l'exploration de l'infiniment petit et considère comme possible d'écrire de grandes quantités d'informations sur de très petites surfaces. Il déclare d'ailleurs : "Pourquoi ne pourrions-nous pas écrire l'intégralité de l'Encyclopaedia Britannica sur une tête d'épingle ". Il ouvre ainsi une réflexion qui conduira aux recherches en nanotechnologie.
Les scientifiques élaborant eux-mêmes des théories et des hypothèses pour le moins surprenantes, la science-fiction s'impose comme genre littéraire. Le terme "science-fiction" lui-même avec pour synonyme et concurrent direct le mot "anticipation". Elle met en scène des univers où se déroulent des faits impossibles ou non avérés en l'état actuel des techniques, mais qui correspondent à des découvertes pouvant advenir un jour. Le progrès technique devient alors un objet fantasmatique où s'expriment toutes sortes d'attentes et d'inquiétudes.

Années 1960
Assemblage de composants électroniques
Le processus de miniaturisation des composants se poursuit, permettant la réduction des coûts de production, tandis que les langages de programmation sont de plus en plus élaborés, grâce à des algorithmes toujours plus sophistiqués. Le processus de commercialisation des ordinateurs s'amorce, mais ne concerne alors que le secteur de l'entreprise.

Émergence d'Internet
En 1961, démarrent les recherches qui aboutiront, vingt ans plus tard, à la naissance d'Internet. Leonard Kleinrock, étudiant au M.I.T., publie une théorie sur l'utilisation de la commutation de paquets pour transférer des données. En 1969, grâce à ses recherches, est conçu le projet ARPAnet (Advanced Research Projects Agency Network), premier "réseau à transfert de paquets". La connexion s'établit entre les laboratoires de quatre grandes universités américaines, pour le compte du Département américain de la Défense. La mise en place du dispositif ARPAnet s'inscrit dans le contexte de la Guerre froide. L'objectif est de créer un réseau de télécommunications militaire à structure décentralisée capable de fonctionner malgré des coupures de lignes ou la destruction de certains systèmes. L'utilisation civile du réseau ARPAnet n'a nullement été envisagée à l'époque où il a été conçu.

Une nouvelle vision du monde
La célèbre citation de Marshall McLuhan, signifiant que l'espace médiatique est désormais intégré dans l'imaginaire collectif (1967).
En 1961, le Soviétique Gagarine effectue le premier vol spatial, mais peu à peu, c'est l'homme du commun qui adopte une nouvelle vision du monde. En 1967, deux ans avant que les Américains ne marchent sur la Lune, le sociologue canadien Marshall McLuhan utilise l'expression "village planétaire" pour exprimer l'idée que tout un chacun va de plus en plus éprouver le sentiment que le monde entier lui est "accessible" et que les médias ne constituent pas un moyen d'information "neutre", mais qu'ils exercent une sorte de fascination sur la conscience et modifient en profondeur le processus de la perception: L'enfant très jeune est comme le primitif : ses cinq sens sont utilisés et ont trouvé un équilibre. Mais les technologies changent cet équilibre ainsi que les sociétés. L'éducation développe un sens en particulier. Hier c'était la vue, par l'alphabet et l'imprimerie. Depuis plusieurs décennies, c'est l'ouïe. Et désormais, c'est notre système nerveux central. "Video-Boy" a été élevé par la télévision. Sa perception est programmée autrement, par un autre média(20).
La même année, et sur un autre registre, l'écrivain français Guy Debord affirme : Toute la vie des sociétés dans lesquelles règnent les conditions modernes de production s'annonce comme une immense accumulation de spectacles(note 17).
Son approche préfigure le concept de monde virtuel qui sera utilisé alors que des millions d'individus délaisseront de plus en plus le "monde réel" pour focaliser leur attention sur ses représentations.

Homme-machine : complicité, rivalité
Les ordinateurs ont été conçus pour jouer les auxiliaires de l'homme, mais l'importance de leurs capacités de mémoire et d'intelligence suscitent tour à tour fascination et inquiétude dans l'imaginaire collectif. Popularisé en 1960 par le neurophysiologiste Manfred Clynes et le chimiste Nathan Kline, le terme "cyborg" renvoie au concept d'humain "amélioré", mi-humain, mi-machine. En 1968, le film de Stanley Kubrick 2001, L'Odyssée de l'Espace met en scène deux astronautes en conflit avec un superordinateur décidé à n'en faire qu'à sa tête.

Années 1970
Le simulateur de vol a constitué le terrain d'expérimentation des premières images de synthèse en temps réel.
Le développement de l'informatique dans tous les domaines de la société (science, économie, armée, santé, finance, commerce...) se traduit par une augmentation sans cesse croissante de la demande en traitement des informations dans les foyers. Les jeux vidéo sont si popularisés que naît une nouvelle industrie : l'industrie vidéoludique. Or, c'est dans le domaine militaire, avec les simulateurs de vol, qu'apparaissent les premières images de synthèse.

Les ordinateurs personnels se multiplient, un système les relie
En 1971, deux événements distincts se produisent et porteront, ensemble, la "révolution numérique" : l'invention du microprocesseur et la mise en réseau d'une vingtaine d'ordinateurs éloignés géographiquement, préfiguration d'Internet (qui ne deviendra opérationnel qu'en 1983). La firme américaine Intel invente le microprocesseur, un processeur dont tous les composants ont été suffisamment miniaturisés pour être regroupés dans un unique boitier. Ce petit objet va bouleverser radicalement la conception des ordinateurs et surtout en réduire considérablement la taille, et, par extension, les coûts. Ainsi, l'accessibilité des produits informatiques est grandement améliorée (d'abord dans les entreprises, puis chez les particuliers). L'ère de la micro-informatique s'ouvre en 1977 avec l'Apple II, qui est l'un des premiers ordinateurs personnels fabriqués à grande échelle. Conçu par Steve Wozniak, il commence sa carrière auprès des particuliers passionnés. En 1979, la sortie du premier tableur, VisiCalc, le fait entrer dans le monde professionnel. Une augmentation spectaculaire de ses ventes fait en très peu de temps à la fois la richesse de la société Apple et la notoriété de la Silicon Valley où elle siège à l'instar d'Intel.; 1971 marque la genèse d'Internet. 23 ordinateurs sont reliés sur ARPAnet et le premier courrier électronique (courriel) est envoyé. L'année suivante naît InterNetworking, un organisme chargé de la gestion d'Internet. Les protocoles TCP-IP sont définis et formalisent les modalités de transfert des données.
Logo de la Commission nationale de l'informatique et des libertés

Informatique et liberté
La fin de la décennie voit poindre les premières inquiétudes relatives à l'impact de la numérisation des fichiers administratifs sur les libertés. En 1978 naît en France la Commission nationale de l'informatique et des libertés (CNIL)) chargée de veiller à ce que l'informatique reste au service du citoyen et qu'elle ne porte atteinte ni à l'identité humaine, ni aux droits de l'homme, ni à la vie privée.
Les sociologues commencent alors à s'inquiéter des conséquences du développement informatique et du progrès technique sur les libertés. En 1977, Jacques Ellul publie Le Système technicien, le second volet de son triptyque consacré à l'étude de la technique qui, selon lui, est désormais constituée en un "système" menaçant les libertés fondamentales puisqu'il "formate", l'ensemble des activités humaines : Un système, c'est un ensemble d'éléments en relation les uns avec les autres de telle façon que toute évolution de l'un provoque une évolution de l'ensemble, toute modification de l'ensemble se répercutant sur chaque élément. Cette interdépendance s'intensifie avec l'informatique : le système technicien est devenu à la société moderne ce que le cancer est à l'organisme : un nouveau milieu, qui pénètre l'ancien, l'utilise, le phagocyte et le désintègre(note 19).
Pour Ellul, l'informatique constitue le noeud de ce système. Elle ne constitue pas un "problème en soi", mais le fait que l'on ne considère pas qu'elle n'est qu'un ensemble de représentations du réel et non le réel lui-même crée une césure entre monde réel et monde virtuel qui, in fine, menace la liberté de l'humanité tout entière si celle-ci ne la repère pas : L'informatique n'est pas une technique comme une autre, elle porte l'ensemble technicien à sa perfection en mettant tous ses éléments en interconnexion. Ce faisant, elle transforme complètement le rapport au réel, en déréalisant tout, en transformant toute chose en signe à consommer, en rendant toute réalité "autre qu'elle-même" : abstraite, lointaine et sans contenu.(note 19)
Années 1980
Un Apple II, un des tout premiers micro-ordinateurs (1977)
Écrans, consoles et baladeurs : individualisme ou sociabilité
En 1981, l'ordinateur personnel fait irruption dans les foyers. Premier concurrent de l'Apple II, l'IBM PC est produit à plusieurs millions d'exemplaires. En 1984, Sony sort le premier baladeur numérique, deux ans après que le disque compact (CD) ait été commercialisé et ait supplanté le disque vinyle. En 1985, la NES, de la société japonaise Nintendo, domine le marché vidéo-ludique.
Face au nombre croissant de personnes s'isolant du réel en préferrant des univers virtuels (ordinateur, console de jeux, baladeurs, etc.), certains philosophes et sociologues s'interrogent. Tandis que Gilles Lipovetsky voit dans les contacts rapprochés avec les écrans l'une des principales raisons de la montée en puissance de l'individualisme, d'autres (notamment au sein de la sociologie des usages) y décèlent au contraire l'éclosion de nouvelles formes de sociabilité.

Rapport aux images : entre fascination et banalisation
Le numérique transforme radicalement le rapport des hommes aux images : Le cinéma exploite de manière de moins en moins décomplexées les algorithmes informatiques, perfectionnés. En 1982, le film Tron est le premier à utiliser l'informatique de manière intensive afin de concevoir des effets spéciaux et un univers virtuel.; La photographie numérique accentue la production et la consommation domestiques des images, ce qui, à terme, provoque leur banalisation. Après que l'entreprise Canon ait commercialisé le premier appareil photo numérique en 1986, la définition des images, la façon de les stocker, de les télécharger ou de les émettre ne cessent de s'améliorer.
Le téléphone mobile... dont peu pourraient se passer de nos jours.

Internet : réseau des réseaux
1983 est une date historique : le protocole TCP-IP est officiellement adopté et le mot "Internet" fait son apparition. 562 ordinateurs sont connectés en août (on en comptera 1000 en 1984, 10 000 en 1987 et 100 000 en 1989). L'année suivante, la société Cisco Systems commence la conception et la commercialisation des premiers routeurs, permettant d'interconnecter divers réseaux entre eux.

Le téléphone mobile
L'année 1983 est marquée par un autre événement majeur : la commercialisation du premier premier téléphone mobile par la firme Motorola.

Informatique et liberté (suite)
En France, comme ailleurs, le fichage électronique n'est pas vécu comme une atteinte aux libertés fondamentales, mais comme une simple commodité. La carte à puce (qui avait été brevetée en France en 1974) est diffusée au grand public comme carte téléphonique : à la fin de la décennie, le GIE Carte bancaire en commande 16 millions d'exemplaires.
En 1982, dans son livre Changer de révolution, Jacques Ellul estime que le micro-ordinateur pourrait servir de vecteur à une véritable et profonde émancipation des hommes, car il favorise à la fois l'expression de leurs idées et leur coordination. Mais il faudrait selon lui agir "avant que la micro-informatique ne soit "prise" (au sens d'une banquise ou d'une mayonnaise) par le système technicien, car alors, il sera rigoureusement trop tard". Six ans plus tard, toutefois, dans Le Bluff technologique, il se ravise : "Actuellement, j'estime que la partie est perdue. Et que le système technicien, exalté par la puissance informatique, a échappé définitivement à la volonté directionnelle de l'homme".

Science-fiction ou technique fiction
En 1984 est édité le roman Neuromancien (titre original : Neuromancer) de William Gibson, premier ouvrage de science-fiction. Il est généralement considéré comme le roman fondateur du mouvement Cyberpunk ayant inspiré n très grand nombre d'oeuvres. La même année sort Terminator, film d'action d'anticipation américano-britannique de James Cameron, dont le personnage principal est un cyborg assassin venu du futur et où il est question d'un système doté d'une d'intelligence artificielle faisant la guerre à l'humanité afin de l'éradiquer et assurer la suprématie des machines. L'oeuvre connaît un succès international, quatre autres épisodes suivront jusqu'en 2015.

Années 1990

Le smartphone : De zéro à plus de 300 millions d'ordinateurs connectés en dix ans
Naissance d'internet en 1990
En 1990, ARPANET disparaît tandis que le World Wide Web, système hypertexte public, fait son apparition. Il permet de consulter, avec un navigateur, des pages accessibles sur des sites. L'image de la toile d'araignée vient précisément des hyperliens qui lient les pages web entre elles. En 1991, l'application Gopher (aujourd'hui disparue) permet d'accéder en ligne à toutes sortes de documents et de les télécharger, ce qui constitue un événement majeur dans le domaine universitaire. En 1992, on dénombre un million d'ordinateurs connectés et 36 millions quatre ans plus tard. Le protocole HTTP devient la lingua franca d'un réseau qui ne compte alors que 130 sites, qui se positionnent souvent en contrepoint des médias traditionnels. Mais très rapidement, cet archipel devient un labyrinthe. En quatre ans à peine, le nombre de sites explose : on en recense rapidement plus d'un million. Dès lors, l'enjeu est de se repérer dans cette masse énorme de données. Amazon est fondé en 1995, Google en 1998 et bientôt s'ouvre la bataille autour des portails d'information".
Conçu par IBM en 1992 et commercialisé deux ans plus tard, le smartphone constitue un objet qui fait le mieux voir l'ampleur des changements ayant cours à l'ère du numérique; tenant dans la main et pouvant être utilisé presque n'importe où, il concentre toutes sortes de fonctions : téléphone, appareil photo, ordinateur, poste de radio, etc.

Intelligence artificielle - dépréciation de l'humain
Parallèlement aux avancées d'internet, se poursuivent les recherches en intelligence artificielle(IA) ce qui inspire bon nombre de futurologues. Ainsi, en 1993, le penseur transhumaniste Vernor Vinge introduit-il le concept de "singularité technologique" pour formuler l'idée qu'un jour viendra où les capacités humaines seront dépassées par celles de l'IA. Comme pour lui donner raison, en 1997, l'ordinateur Deep Blue (conçu par IBM) gagne une partie d'échecs contre Garry Kasparov, champion du monde en titre.
La même année, l'industrie vidéoludique génère pour la première fois un revenu plus important que lecinéma.
À mesure que les matériels se perfectionnent et se multiplient, quelques pathologies se développent ; en premier lieu, la dépendance.
Les nouvelles technologies permettent de transmettre des ordres boursiers de plus en plus rapidement et en nombre croissant, ce qui provoque de nombreux dysfonctionnements dans le monde de la finance Diagramme montrant la toxicité des flux d'ordres lancés le 6 mai 2010
Les espoirs misés sur les technologies attisent la finance et bouleversent l'économie
Le progrès technique façonne alors littéralement l'économie : la multiplication des outils, le fait qu'ils sont de plus en plus sophistiqués et réunis en réseaux stimulent la "nouvelle économie", dont les maîtres mots sont "innovation" et "croissance". Solange Ghernaouti-Hélie et Arnaud Dufour décrivent le moment d'emportement de l'économie qui débouchera, en mars 2000, sur la bulle internet : "Toute la seconde moitié des années 1990 est marquée par une agitation médiatique sans précédent autour de l'internet, puis de ses dérivés, notamment le commerce électronique. Tour à tour crédité du meilleur comme du pire, l'internet fascine, suscite toutes sortes de convoitises et inquiète en même temps. Dès lors, les milieux financiers investissent massivement dans les sociétés liées à l'informatique, en espérant réaliser des gains importants sur ce marché prometteur mais souvent mal compris. Une génération d'entreprises émerge entre 1996 et 2000 pour offrir des services sur l'internet (fourniture de logiciels, moteurs de recherche, portails, sites d'information, magazines électroniques, commerce en ligne). L'intégration du suffixe ".com" de leur adresse web dans leur nom d'entreprise fait naître l'expression "dotcom" pour les désigner. Les premiers succès de financement et d'introduction en bourse survalorisent certaines dotcom et créent le mouvement de la nouvelle économie, souvent comparé à la ruée vers l'or. Ce phénomène, amplifié par un indéniable effet de mode, pousse alors certains investisseurs à spéculer sur la croissance rapide de l'internet et sur la génération exponentielle de revenus. Cela permet à de nombreuses jeunes entreprises innovantes (start up) de trouver des financements. Ces prévisions de croissance se sont par la suite avérées surévaluées. Dès la fin 1999, certains analystes des domaines technologiques et financiers commencent à prendre leur distance par rapport à ce qu'ils perçoivent comme un excès spéculatif dans la nouvelle économie."

Science fiction ou technique fiction (suite)
En 1999 sort Matrix, film australo-américain qui connaît un succès considérable et qui raconte l'histoire d'un jeune informaticien contacté, via son ordinateur, par ce qu'il pense être un groupe de hackers, lesquels lui font comprendre que le monde dans lequel il vit n'est qu'un monde virtuel dans lequel les êtres humains sont gardés sous contrôle. Le film le décrit comme un nouveau messie : "l'Élu" qui peut sauver l'ensemble des êtres humains du joug des robots. Deux autres épisodes suivront en 2003.

Années 2000
Logo du label "zone d'activité très haut débit"
Internet domestiqué
En 2000, alors qu'Internet passe au haut débit, 368 millions d'ordinateurs sont connectés dans le monde. Le réseau se démocratise, un grand nombre d'individus se l'approprient, ils ouvrent leurs propres sites, leurs blogs, y créent directement de nouveaux outils sans nécessairement posséder de compétences particulières en informatique. Ce nouvel essor est promu sous l'appellation web 2.0. On ne parle plus, comme dans la décennie précédente, d'autoroutes de l'information mais de "société de communication" ou de web participatif.

Quand le numérique fait corps avec les autres technologies
En 2000, l'application de l'informatique et d'un micro-opto-electro-mechanical system (MOEMS) à l'industrie du cinéma permet la réalisation par le français Philippe Binant de la première projection cinéma numérique européenne.
En 2001, dans un rapport qu'ils remettent à la National Science Foundation, les Américains William S. Bainbridge et Mihail Roco créent l'acronyme NBIC pour désigner ce qu'ils considèrent comme la "nécessaire convergence" entre les nanotechnologies, les biotechnologies, l'informatique et les sciences cognitives, c'est-à-dire l'interconnexion entre l'étude de l'infiniment petit, la fabrication du vivant, les recherches en intelligence artificielle et celles menées sur le cerveau humain. Cette convergence exigeant des mises de fonds considérables, des stratégies de développement sont conjointement élaborées par les États et le monde industriel. De même que, chez les individus, les NTIC tendent à briser les frontières traditionnelles entre vie publique et vie privée, dans la sphère économico-politique, elles contribuent à associer de plus en plus étroitement le secteur public et le secteur privé.

De la recherche de l'efficacité maximale au désordre généralisé : la bulle internet
Salle du Nasdaq.
En 2000, la "révolution numérique" censée symboliser l'émancipation de l'humanité se montre sous le visage du chaos : la bulle internet explose : Neutre Dès la fin mars, l'indicateur du Nasdaq s'effondre, perdant près de la moitié de sa valeur en quelques mois. En France, l'indice du Nouveau marché s'écroule lui aussi, avec à peine quelques mois de décalage. Cette rupture de la croissance des marchés, qualifiée par quelques-uns d'e-krach(note 25), affecte immédiatement l'ensemble des dotcoms, en différant et en réduisant leur possibilité de lever des capitaux. De nombreux projets sont stoppés ou réduits et les start-up les plus fragiles, souvent incapables de générer des profits suffisants, font faillite. Par effet domino, certaines entraînent dans leur chute leurs partenaires et leurs investisseurs. Dès lors, ce secteur connait une restructuration profonde, affectant par contagion l'économie dite traditionnelle et notamment les fournisseurs de matériel informatique, qui voient les commandes s'effondrer en même temps que réapparaissent sur le marché de l'occasion les machines récemment acquises par les start-up fermant leurs portes.
Alors que les entreprises réalisent de bonnes affaires, les investisseurs exagèrent l'importance du "très long terme" dans leurs estimations et négligent de prendre en compte le fait que la plupart d'entre elles consomment trop vite leur capital. L'hypervalorisation des acteurs de cette économie est souvent sans rapport avec la réalité des indicateurs fondamentaux qui constituent la valeur d'une entreprise. Dans bien des cas, les calculs de valorisation ne peuvent s'appuyer sur des bénéfices réels et doivent reposer sur des chiffres hypothétiques auxquels sont appliqués des taux de croissance qui ne le sont pas moins. Logo de Facebook, l'un des premiers réseaux sociaux
Si la bulle financière est fatale à bon nombre de dirigeants de start-ups, d'autres s'en sortent et vont même faire fortune. En 2001, Jimmy Wales et Larry Sanger fondent Wikipédia, première encyclopédie collaborative. Puis les premiers réseaux sociaux font leur apparition : en 2004, Mark Zuckerberg crée Facebook ; deux ans plus tard, Jack Dorsey met en place Twitter... après avoir irrigué la sphère professionnelle, internet s'immisce dans tous les domaines de la vie privée.

Nuisances, désagréments, dysfonctionnements, stress, inquiétudes, aberrations...
Par delà les simples nuisances et désagréments spécifiques à internet (ex. multiplication des courriels, nécessité des mises à jour...) ainsi que des dysfonctionnements à répétition (virus, spam), planent de réelles inquiétudes : les fondements traditionnels de l'éthique et de la liberté semblent menacés aussi bien par les institutions étatiques et les fournisseurs d'accès (par exemple dans le cas du déni de service) que par de simples particuliers, voire des robots.
Les réactions sur ces questions sont les plus multiples. En 2008, en France, le projet de fichier de police informatisé Edvige soulève un tollé dans une partie de l'opinion publique. À l'inverse, certains individus adoptent une posture de servitude volontaire vis-à-vis des TIC, notamment du phénomène de la radio-identification.
Caméras de vidéosurveillance: De l'État et des institutions publiques vient la première source de crainte. L'archivage des images de vidéosurveillance, le fichage biométrique, la capacité de localiser tout individu porteur d'un smartphone et d'activer secrètement celui-ci pour mettre son utilisateur sous écoute... Consécutifs aux attentats du 11 septembre 2001, ces dispositifs sont essentiellement justifiés par des arguments sécuritaires, face aux risques d'attentats terroristes, mais n'en sont pas moins vécus par un certain nombre d'individus comme des instruments de contrôle social. En réaction, certains d'entre eux, rompus aux techniques informatiques, créent des sites ayant fonction de lanceurs d'alerte, le plus fameux d'entre eux, à partir de décembre 2006, étant Wikileaks créé par l'Australien Julian Assange.; Les robots peuvent également semer le trouble comme on l'observe avec l'algotrading, forme de trading nécessitant l'utilisation de plates-formes électroniques pour la saisie des ordres de bourse et qui permet à un algorithme de prendre différentes décisions (l'instant d'ouverture ou de clôture, le prix et le volume de l'ordre) sans la moindre intervention humaine, et ceci parfois en pleine période d'instabilité financière.; Des menaces émanent également d'individus isolés pratiquant le piratage de données numériques : profitant d'un certain nombre de connaissances en informatique, ils parviennent à détourner à leur profit des sommes d'argent par simples virements bancaires ou de déplacer des fonds vers des paradis fiscaux afin de se dérober à l'impôt.
Puce électronique sous-cutanée (par RFID) implantée chez les carnivores domestiques et comparée à la taille d'un grain de riz. S'exprime une autre inquiétude que le système RFID (de l'anglais radio frequency identification) développé en 2007 pour pouvoir suivre à la trace des animaux de bétail puisse être appliqué un jour sur les humains à leur insu, que ces intrusions soient le fait d'États ou de particuliers, du fait que les puces permettant la géolocalisation sont minuscules et accessibles à n'importe qui. De même, la commercialisation des drones permet à n'importe qui de surveiller aisément n'importe qui à son insu.; Le plus surprenant sans aucun doute est qu'un phénomène comme la radio-identification fascine certains esprits autant qu'il en inquiète d'autres. En 2005, un rapport du Groupe européen d'éthique des sciences et des nouvelles technologies révèle que des humains se font implanter délibérément des micropuces permettant de les localiser à tout moment.

Science-fiction ou technique fiction (suite)
En 2009, sort le film de science-fiction Avatar, l'un des plus coûteux de toute l'histoire du cinéma. Il devient, après seulement six semaines d'exploitation, le plus gros succès de l'histoire du cinéma, battant Titanic, également réalisé par James Cameron. Il raconte l'histoire d'un homme dont la conscience est téléchargée dans le clone d'un habitant d'une lointaine planète.

Années 2010
Les premières années de la décennie 2010 sont caractérisées d'une part par le fait que ne cesse de s'estomper la traditionnelle distinction entre vie privée et vie publique, d'autre part que, le flux des informations circulant sur internet ne cessant de croître, les bases de données sont de plus en plus volumineuses et coûteuses en énergie : c'est le phénomène "big data" (mégadonnées en français).

Vie publique, vie privée : le grand mélange
En 2010, Mark Zuckerberg, fondateur de Facebook, estime que les 350 millions d'utilisateurs de son site n'attachent plus autant d'importance à la protection de leurs données personnelles et considère que "la protection de la vie privée n'est plus la norme" : Les gens sont désormais à l'aise avec l'idée de partager plus d'informations différentes, de manière plus ouverte et avec plus d'internautes. (...) La norme sociale a évolué. (...) Les gamins se sont toujours préoccupés du respect de leur vie privée, c'est juste que ce qu'ils entendent par 'vie privée' est très différent de ce que cela représente pour les adultes. (...) En tant qu'adultes, nous pensons que notre maison est un espace privé. Or, pour les jeunes, ce n'est pas le cas : ils ne peuvent pas contrôler qui entre ou sort de leur chambre. Pour eux, le monde en ligne est davantage privé, parce qu'ils ont davantage de contrôle sur ce qui s'y passe. Les big data : des masses phénoménales d'informations circulant à travers le monde
Big data, Big Brother...(modifier modifier le code)
L'avènement des big data est lié au fait que l'ensemble des informations stockées et circulant dans le monde est devenu si volumineux qu'il exige de nouveaux outils. Le cloud computing exprime un basculement de tendance : au lieu d'obtenir de la puissance de calcul par acquisition de matériel et de logiciel, les consommateurs se servent de la puissance mise à disposition par les fournisseurs d'accès. Le symbole de ce virage est le centre de données, extraordinairement coûteux en énergie: L'inflation exponentielle des données de toute nature traitées par les entreprises est devenue aujourd'hui une vraie problématique. De l'infobésité galopante dont on parlait il y a à peine deux-trois ans, on est passé au déluge planétaire d'informations. Le phénomène Big Data s'amplifie si vite que l'on n'arrive plus à suivre l'évolution des nouvelles unités de mesure : les exaoctets (1018 octets), les zettaoctets (1021), les yottaoctets (1024)... Mais, si on a jusqu'à présent surtout cherché à quantifier le phénomène en termes de volumétrie, on ne s'était encore guère inquiété du coût que représente le traitement par les entreprises de cette masse d'informations(33).
La considérable avance prise par les États-Unis en matière technologique les met en situation de supériorité sur le reste de la planète, comme le montre les nombreuses révélations d'Edward Snowden à partir de 2013, qui révèle l'ampleur des programmes d'espionnage menés par l'Agence nationale de la sécurité américaine sur l'ensemble de la planète. Les responsables politiques ont de plus en plus de mal à exprimer leur impuissance face à la surveillance globale de la sphère numérique.

Poussières et objets intelligents
Les avancées dans les domaines de la robotique, de l'intelligence artificielle et des nanotechnologies ont pour effet de rendre l'environnement des hommes intelligent. (Elles) permettent de produire des entités informatiques communicantes si petites qu'elles sont dénommées poussières intelligentes. Elles autorisent une véritable intégration (fusion) du monde de l'informatique dans le monde du vivant. Déjà, l'internet des objets émerge progressivement et fait référence au fait que des objets courants, comme des équipements électroménagers, comportent des composants capables de prendre des décisions en fonction de leur état et de leur environnement. Exemple : la voiture peut dialoguer avec le téléphone portable et couper le son de l'autoradio lors de la prise d'un appel.
Réception
Le mot "révolution" étant fortement connoté, l'expression "révolution numérique" ne fait pas consensus. Certains voient dans le progrès technique le vecteur et la condition même du progrès social ; d'autres y décèlent au contraire l'expression d'une tendance prométhéenne et le signe d'une aliénation conduisant l'humanité à sa perte.
Entre ces deux positions extrêmes, différentes attitudes et grilles de lecture sont repérables qui, chacune à sa matière, invitent à repenser l'éthique et réévaluer les notions de modernité, de liberté, de croyance, de lucidité et de responsabilité.

Éloges
L'expression "révolution numérique" a été créée et est utilisée par des penseurs de sensibilité technophile et qui identifient le progrès technique au progrès de l'humanité : Il n'est sans doute pas exagéré de comparer la révolution numérique d'aujourd'hui à la révolution industrielle d'hier. De nouvelles barrières aux échanges sautent. Les structures, les hiérarchies et les divisions habituelles se fragilisent. Un monde dans lequel communiquer à des milliers de kilomètres et avec des milliers d'interlocuteurs devient possible sans délai, et où cela ne coûte pratiquement rien, ne fonctionne certainement plus comme le monde auquel nous sommes habitués.
Elle est également célébrée par les milieux libéraux qui voient en elle le moyen principal de stimuler le système capitaliste : La révolution numérique de l'université constitue un formidable enjeu. Sur le plan économique, l'éducation est le principal levier pour dégager des gains de productivité dans un système de production dominé par la connaissance.. H+, le symbole du transhumanisme.
Mais c'est chez les penseurs transhumanistes que s'exprime l'éloge le plus exalté de la "révolution numérique" (et du progrès technique, de façon plus générale) puisqu'ils attendent de la convergence NBIC qu'elle transforme radicalement l'espèce humaine : Le transhumanisme est plus qu'une simple croyance abstraite que nous sommes sur le point de transcender nos limitations biologiques au travers de la technologie. C'est aussi une tentative pour réévaluer la définition entière de l'être humain comme on la conçoit habituellement.
Les transhumanistes attendent en particulier des avancées en informatique que l'on puisse un jour télécharger intégralement le contenu d'un cerveau : Si nous pouvions scanner la matrice synaptique d'un cerveau humain et la simuler sur un ordinateur, il serait possible pour nous de migrer de notre enveloppe biologique vers un monde totalement digital. En s'assurant que nous ayons toujours des copies de remplacement, nous pourrions effectivement jouir d'une durée de vie illimitée.
. Aussi hallucinante que puisse paraître cette idée, qui trouve son origine dans les livres de science-fiction, elle tend aujourd'hui à être appliquée. Ainsi, le "Projet du cerveau humain" (que l'Union européenne soutient financièrement depuis 2013 à hauteur d'un milliard d'euros) vise à simuler le fonctionnement du cerveau grâce à un superordinateur. L'argument avancé est de développer de nouvelles thérapies sur les maladies neurologiques.
Les milieux religieux ne sont pas forcément les plus critiques envers la "révolution numérique". En janvier 2014, appelant les catholiques à être des "citoyens du numérique", le pape François qualifie internet de "don de Dieu".

Adaptations
Partout dans le monde, l'adaptation au numérique est considérée comme une chose nécessaire. Écoliers du Rwanda
Les penseurs libéraux perçoivent la "révolution numérique" comme un fait accompli et allant de soi. Ils n'en sous-estiment pas les effets contre-productifs, voire pervers, mais ils considèrent que les hommes l'ayant "adopté", ils doivent impérativement prendre le parti de s'y "adapter" pour en retirer le meilleur : Avant, nous allions sur internet, maintenant, nous sommes dedans. Nous avons adopté les nouvelles technologies et elles ont tout bouleversé : les démocraties et les dictatures, la paix et la guerre, les États et les sociétés civiles. Elles servent à la fois d'outils de libération et d'oppression, de partage et d'exclusion. La révolution numérique apporte peut-être autant de changements que l'avènement de l'agriculture. Plus de deux milliards d'humains sont aujourd'hui connectés à Internet, faisant basculer dans le champ politique la question numérique, jusqu'ici cantonnée à la technique et à l'économie. La crise donne aux hommes de nouvelles occasions de se révolter, les réseaux leur offrent de nouveaux moyens de le faire. (...) L'avenir appartient à ceux qui s'en saisissent, non à ceux qui le refusent.
Vue sous cet angle, "la révolution numérique" est un processus qui, étant déjà enclenché, agit sur les hommes comme une "main invisible" (au sens qu'Adam Smith donnait à cette expression pour définir le marché) : "elle ne se refuse pas" signifie qu'il n'y a pas lieu d'en critiquer les fondements. "S'en saisir", en revanche, c'est se montrer technophile non pas par idéalisme (technolâtrie) mais par pragmatisme, position que résume l'adage populaire "on n'arrête pas le progrès" et qui est aujourd'hui dominante.
De fait, l'économie planétaire étant elle-même tout entière soumise à la doctrine libérale, l'ensemble de la classe politique (de la droite institutionnelle à la social-démocratie) ainsi que les principaux acteurs économiques s'inscrivent dans cet état d'esprit. Les pouvoirs publics autant que les fournisseurs d'accès, entendent réduire la fracture numérique et élargir indéfiniment l'accès à internet : les premiers invoquent des motifs égalitaires, les seconds entendent gagner de nouvelles parts de marchés, mais les uns et les autres agissent de concert. La "révolution numérique" ne se développe donc plus comme elle s'était amorcée, de façon improvisée, mais sur la base d'une étroite collaboration entre l'État et le monde de l'industrie. Non seulement dans le domaine de l'informatique, mais également ceux des nanotechnologies, des biotechnologies et des sciences cognitives. La convergence NBIC renforce l'esprit de consortium entre les secteurs public et privé, servant de base à des projets extrêmement ambitieux et coûteux.
Auteur de Le jour où mon robot m'aimera, Serge Tisseron incarne assez bien la tendance technophile en France
Un très grand nombre de penseurs en sciences humaines, que ce soit en sociologie, en psychologie ou en philosophie, s'adaptent également à la "révolution numérique". Leur approche se résume à l'adage "la technique n'est ni bonne ni mauvaise, tout dépend de l'usage que l'on en fait". En France, Serge Tisseron est le plus représentatif de cette "sociologie des usages". Celle-ci s'est développée au début des années 1980 avec le besoin d'étudier les TIC dans le monde du travail puis dans le contexte de la vie privée. Percevant l'avènement du numérique comme facteur de changements fondamentaux dans les domaines culturel, cognitif et psychologique, Tisseron propose l'expression culture de l'écran, en regard de celle de culture du livre. Selon lui, il n'y a pas lieu de dévaloriser la première par rapport à la seconde. Il considère par exemple que le choix de pseudos et d'avatars sur les forums et dans les jeux vidéo relève d'une quête expérimentale et constructive de son identité.
Plus explicite encore de cette adaptation à la "révolution numérique", le philosophe Michel Serres s'accommode non seulement des bouleversements intergénérationnels causés par la révolution numérique, mais il y voit le signe d'une avancée de l'humanité : La science, c'est ce que le père enseigne à son fils. La technologie, c'est ce que le fils enseigne à son papa.
S'adapter à la "révolution numérique", selon ces penseurs, revient à s'adapter au progrès technique dans son ensemble : on ne peut critiquer celui-ci que depuis ses conséquences (lesquelles doivent être corrigées lorsqu'elles sont négatives et anticipées pour qu'elles ne le deviennent pas, selon le principe de précaution). En revanche, les causes ne sont pas critiquables : "on n'arrête pas le progrès" signifie que l'on part du principe que l'homme moderne est suffisamment adulte pour le contrôler, depuis une éthique qu'il se forge lui-même librement.
Or c'est précisément ce postulat que contestent les penseurs critiques (cf paragraphe suivant).

Critiques
En 1988, dans Le bluff technologique, Jacques Ellul écrit : Le système technicien, exalté par la puissance informatique, a échappé définitivement à la volonté directionnelle de l'homme.
Le phénomène "révolution numérique" participe du phénomène "progrès technique" qui, au XXe siècle, a provoqué différentes réactions, parmi lesquelles celle d'Herbert Marcuse, pour qui la "technoscience" est un processus n'ayant d'autre finalité que de servir le capitalisme, et surtout celle de Jacques Ellul, qui voit dans l'adaptation à "la technique" précédemment décrite la marque d'un conformisme d'un nouveau type : L'homme est aujourd'hui tellement fasciné par le kaléidoscope des techniques qui envahissent son univers qu'il ne sait et ne peut vouloir rien d'autre que de s'y adapter complètement.
Ellul est décédé en 1994, au moment où commençait à se généraliser l'expression "révolution numérique", mais son oeuvre est éclairante dans la mesure où elle comprend trois analyses détaillées du concept de révolution et trois autres du phénomène technicien. Il perçoit dans l'association des mots "révolution" et "technique" une contorsion du langage : "l'homme moderne" s'évertue à croire qu'il dirige et contrôle un processus qui, en définitive, le submerge et le contraint à se plier à ses exigences. Et s'il sacralise la technique, c'est parce qu'elle est porteuse d'une valeur qui surplombe toutes les anciennes valeurs (raison, liberté, égalité...) et se substitue peu à peu à elles. Tant qu'il ne l'a pas admis et compris, il ne peut prétendre contrôler le phénomène technique par les seules vertus de sa volonté. Aucun fait social humain, spirituel, n'a autant d'importance que le fait technique dans le monde moderne. (...) la Technique a progressivement gagné tous les éléments de la civilisation". (...) Elle constitue la préoccupation de l'immense majorité des hommes de notre temps de rechercher en toutes choses la méthode absolument la plus efficace
Selon les membres de l'association Technologos, le fait que bon nombre de discours en faveur des "nouvelles technologies" fassent aujourd'hui état d'une obligation de s'y adapter accrédite la thèse ellulienne que ce qui est généralement présenté comme un progrès relève en définitive d'une aliénation. Exemple : L'introduction des NTIC (Nouvelles Technologies de l'Information et de la Communication) dans l'éducation oblige élèves, enseignants et parents à apprendre et à communiquer autrement.
L'analyse ellulienne invite à repenser le phénomène numérique dans le cadre plus large du progrès technique et celui, plus étendu encore, de la modernité : quelles sont les motivations profondes de l'être humain lorsqu'il étend et perfectionne sans cesse le parc de ses équipements Sont-elles conscientes et assumées ou bien relèvent-elles de l'idéologie
Sur le plan industriel, la révolution numérique, aussi parfois qualifiée de 4e Révolution Industrielle, se heurte à certaines réserves quant à la capacité des machines à remplacer les Hommes. Alors qu'Erik Brynjolfosson et Andrew McAfee, dans Le Deuxième Âge de la Machine annoncent que les robots vont remplacer les Hommes, un certain nombre d'intellectuels et d'industriels pensent que l'Homme ne peut être remplacé. Le robot ne serait voué qu'à faire évoluer leur métier. C'est notamment l'avis de Bruno Bonnel et Yann Le Galès, rédacteur en chef adjoint au service économie du Figaro : "Il y avait moins d'emplois dans l'Internet en 1996 quand j'ai créé Infonie, le premier fournisseur d'accès Internet que dans la robotique aujourd'hui". Ainsi, "Hier, comme aujourd'hui, ce sont les hommes qui dirigent les machines ; sur des marchés internationaux volatiles, il faut certes des cadences de production ajustables et donc des robots performants, mais aussi, mais surtout, des hommes pour prendre les bonnes décisions d'ajustement".

Questionnements
Les analyses telles que celles de Jacques Ellul restent encore assez peu étudiées. Le politologue Patrick Troude-Chastenet explique cette faible réception par le fait que, bien qu'Ellul ait mené une carrière universitaire, son discours s'écarte sensiblement des codes traditionnels marqués par l'objectivisme caractéristique des sciences sociales.
Toujours est-il que l'essor du numérique est le vecteur d'un paradoxe : il génère autant de dysfonctionnements (aux plans écologique, politique, économique, juridique, psychosocial, etc.) qu'il est régulièrement présenté comme "révolutionnaire". Ce qui pose différentes questions : que nous apporte réellement cette révolution Mais aussi : de quoi nous prive-t-elle et de quoi nous menace-t-elle
Dominique Wolton, spécialiste des rapports entre techniques et société : le numérique est tout sauf neutre.
En 1999, quelques mois avant que n'éclate la bulle internet, Dominique Wolton (spécialiste des médias et des rapports entre sciences, techniques et société) ne se berce pas d'illusions quant à la prétendue "révolution numérique" : Internet est-il une révolution aussi importante que la radio dans les années 20 et la télévision dans les années 60 On peut en douter. Pour penser les nouveaux médias, il faut bousculer le discours dominant, qui leur est benoîtement favorable, et les replacer dans une théorie générale de la communication. Il est donc urgent d'ouvrir le débat en rappelant notamment certaines contradictions liées à la "révolution de la communication". À quoi reconnaît-on l'idéologie technique Au fait de traiter de pessimiste ou de conservateur, en tout cas d'adversaire du "progrès", quiconque remet en cause le sens et l'utilité des nouveaux médias, et réclame une réflexion et des réglementations. Aucun système technique n'a jamais donné naissance à un modèle de société ; c'est même tout le contraire : plus il y a de systèmes d'information automatisés, plus il faut des lois pour éviter les abus de la cybercriminalité. La loi n'entrave pas la liberté de communication ; elle évite, au contraire, de confondre performance technique et contenu des activités. (...) Faudra t-il demain un "Titanic de la cyberculture" pour que les États prennent conscience des risques que ces systèmes d'information font peser sur les libertés fondamentales (...) L'Occidental a mis des siècles à se libérer de toutes les tutelles : religieuses, politiques, sociales, militaires... Enfin libre de penser, de circuler et de s'exprimer, il décide aujourd'hui de s'enfermer dans les mille fils de la communication technique. Il est constamment rattaché à elle, joignable en permanence, par portable, fax, téléphone, e-mail (...) Après nous être "en-mailés" au nom de la liberté et du progrès, ne nous faudra t-il pas, au nom de cette même liberté et de ce même progrès, apprendre à nous "dé-mailer"
Des phénomènes tels que la vidéosurveillance, le fichage biométrique et la géolocalisation suscitent l'inquiétude qu'émerge un nouveau type de totalitarisme, tel que l'écrivain George Orwell, en 1949, dans son roman d'anticipation 1984, en faisaient la description. De fait, les révélations faites en 2013 par l'informaticien Edward Snowden, ancien employé de la CIA et de la NSA, confortent la théorie "Big Brother". Selon le romancier Marc Dugain et le journaliste Claude Labbé, "il existe un pacte secret scellé par les big data avec l'appareil le plus puissant de la planète. L'anthropologue Paul Jorion considère toutefois que le problème ne se pose pas de façon unilatérale : si l'État peut s'immiscer dans les communications des particuliers, l'inverse est vrai également. Ce qui, selon lui, se profile par conséquent au XXIe siècle, c'est une "guerre civile numérique".
La révolution numérique, c'est également l'extraction de métaux rares, nécessaires à la fabrication des composants électroniques, et la mise au rebut d'appareils de plus en plus rapidement obsolètes.
Les effets négatifs de la "révolution numérique" sur l'écologie planétaire sont assez rarement soulignés. Étant donné qu'elle dématérialise les activités humaines, celle-ci est généralement considérée comme apte à réduire l'impact de la croissance sur la biosphère, voire résoudre la crise environnementale. Certains, toutefois, estiment le contraire : "si le monde numérique semble virtuel, les nuisances qu'il provoque, elles, sont bien réelles : la consommation des centres de données dépasse celle du trafic aérien, une recherche sur Google produit autant de CO2 que de porter à ébullition de l'eau avec une bouilloire, la fabrication des équipements nécessite l'utilisation d'une quantité considérable de matières premières, l'obsolescence des produits ne cesse d'accroître la mise au rebut de composants électroniques extrêmement polluants".
La "révolution numérique" bouleverse complètement les cadres juridiques traditionnels. La mise en ligne d'oeuvres artistiques (photos, films, livres, musique...), par exemple, oblige une révision complète de la notion de propriété intellectuelle. Internet, de façon générale, inaugure de nouveaux types de crimes et délits : les infractions aux cartes bancaires (piratage), le blanchiment d'argent et l'évasion fiscale (du fait qu'il est techniquement possible à un simple particulier de rendre opaque un certain nombre de transactions) et développe certains pans de la criminalité "classique" : incitation à la haine raciale ou au terrorisme, pédophilie... Par voie de conséquence, les professionnels de la police et de la justice sont donc de plus en plus formés aux techniques informatiques, qui sont toujours plus nombreuses et complexes du fait que les cybercriminels eux-mêmes progressent en niveau d'expertise.
La généralisation d'Internet et du téléphone portable, tant dans le monde du travail que dans celui de la vie quotidienne, fait apparaître un certain nombre de risques sanitaires (effets nocifs des ondes électromagnétiques sur le cerveau) et de nuisances. En particulier, certains penseurs considèrent la multiplication des messages comme étant chronophage, source de dépendances ou de stress, destructrice de liens sociaux, malgré le succès des réseaux sociaux et des forums électroniques et du fait d'une confusion généralisée entre le monde réel et ses représentations. La généralisation de l'usage de l'anonymat sur internet invite à repenser la notion de responsabilité tandis que l'expansion des comportements addictifs oblige à reconsidérer celle de liberté, que la prolifération des informations (vérifiés ou non) rend toujours plus difficile l'exercice de l'esprit critique et que le libre accès aux sites pornographiques, malgré l'usage des filtres, bouleverse l'ensemble du champ éthique. À Nantes, en 2014 (pour la première fois en France), le milieu universitaire traite la question de l'accès au numérique en termes d'addiction. Mais cette formation est assurée sous la forme... d'un cours en ligne.
Marc Dugain et Claude Labbé considèrent que les différents types de dysfonctionnements (juridiques, économiques, écologiques, sanitaires...) liés à ce qu'ils appellent "la dictature invisible du numérique" ont pour conséquence un relâchement lent et progressif du questionnement éthique : "cette révolution numérique ne se contente pas de modeler notre mode de vie vers plus d'information, plus de vitesse de connexion, elle nous dirige vers un état de docilité, de servitude volontaire, de transparence, dont le résultat final est la disparition de la vie privée et un renoncement irréversible à notre liberté". Leur position rejoint celle de Jacques Ellul tout en s'opposant pourtant diamétralement à elle, Ellul considérant en effet que le relâchement éthique n'est pas tant une conséquence qu'une cause : "l'homme (moderne) n'est pas du tout passionné par la liberté, comme il le prétend. La liberté n'est pas un besoin inhérent à la personne. Beaucoup plus constants et profonds sont les besoins de sécurité, de conformité, d'adaptation, de bonheur, d'économie des efforts (...) et l'homme est prêt à sacrifier sa liberté pour satisfaire ces besoins".

Engagements
Une grande majorité des partis politiques institutionnels - hantée par "le spectre du chômage" - voit dans les "nouvelles technologies" le principal levier de la croissance, le secteur le plus générateur d'emplois. De nombreux débats ont lieu sur les questions de bioéthique, de propriété intellectuelle et sur les moyens de gouverner internet, notamment pour contrer le phénomène de de la cybercriminalité. Mais ils restent "internes", confidentiels, réservés aux experts et aux technocrates, ne donnant lieu à aucune consultation démocratique du fait que la majorité des individus concentrent leur intérêt sur la politique spectacle.
Les associations militantes, notamment dans la mouvance altermondialiste, comme Attac, ne s'engagent pas davantage sur la question du numérique et des technologies en général. Tout au plus est dénoncée la bienveillance avec laquelle certains gouvernements, toutes sensibilités confondues, considèrent que les entreprises high-tech et la façon dont elles dirigent le secteur de la recherche (qui relève du service public) s'alignent sur leurs attentes, alors que celles-ci n'ont d'autres objectifs que d'accumuler les profits. La fascination des individus devant les smartphones, tablettes, jeux vidéo et autres, est reconnue, mais il semble que l'on cultive parfois le voeu qu'à force d'éducation populaire, les consommateurs deviennent "consom'acteurs" (sic) et citoyens.
Quelques sociologues s'efforcent d'analyser l'absence d'engagement critique de "la gauche" sur les questions relatives aux répercussions de la technique sur le quotidien : Les intellectuels et les jeunes qui les écoutent (...) ne voient pas le danger d'une évolution qui fragilise notre vie quotidienne, en nous mettant à la merci des fluctuations de l'économie et de processus sociotechniques sur lesquels nous n'avons aucune prise(note 53).
La question de l'omniprésence du numérique (et du progrès technique en général) ne suscite finalement que quelques prises de position de la part d'associations ou de groupements militants. En France, on peut repérer deux courants assez opposés, l'un plutôt favorable à la "révolution numérique", l'autre au contraire très critique à son endroit.
Un courant libéral, qui aborde la question des technologies sans remettre en cause les cadres idéologiques dans lequel elles s'inscrivent, à savoir le libéralisme et le productivisme. À l'intérieur de ce courant, on distingue deux tendances : Les "technolâtres" (du grec latreia : "adoration") : malgré les risques qu'il soulève et les nuisances qu'il génère, le progrès technique est considéré comme une authentique émancipation de l'homme, il est synonyme de progrès social. Ce point de vue est défendu par l'association Technoprog!, qui trouve ses origines dans le mouvement transhumaniste, apparu dans la Silicon Valley dans les années 1980.; Les "technophiles" (du grec philein : "aimer") : la technique est évaluée autant du point de vue de ses avantages que celui des risques et dangers. Mais précisément pour parer aux premiers et éviter les seconds, il convient de lui porter un intérêt tout particulier. C'est la posture défendue entre autres par la FING (Fondation Internet Nouvelle Génération), association créée en 2000 dont les objectifs affichés sont : "mobiliser autour des technologies à venir", "favoriser l'émergence d'idées et de projets innovants", "encourager l'appropriation de l'innovation et les partenariats". Cette posture s'inscrit dans l'idée qu'il convient de "prendre part aux nouveaux débats éthiques et sociétaux" mais sans qu'il soit clairement précisé qui serait à l'origine de ces débats, dans quel but et selon quels principes éthiques. Créé en 2005, le think tank Renaissance numérique "défend l'internet et le numérique citoyen pour permettre au plus grand nombre de foyers français d'avoir accès à internet".
Un courant critique, ou technocritique (du grec krinein : "trier"), qui traite au contraire des technologies en les contextualisant dans le champ de l'idéologie dominante, le libéralisme économique. Là également, on repère deux orientations : Les "technophobes" (du grec phobos : "peur") : le progrès technique est intrinsèquement générateur d'aliénation, raison pour laquelle il y a tout lieu de le craindre. Directement inspiré du luddisme, ce courant est porté par le collectif anonyme grenoblois Pièces et Main d'Oeuvre (fondé en 2000), le groupe Marcuse (également un collectif anonyme) et le Journal La Décroissance. Le ton adopté est "provocateur" dans le but revendiqué de "provoquer un éveil des consciences". Par exemple : La technologie est la continuation de la guerre. Les "technologies" finissent par être présentées comme si elles étaient des objets agissant par eux-mêmes et que nul sujet n'en était à l'origine : Si l'alphabétisation fut bien souvent la compagne de l'émancipation, les technologies contemporaines préparent et organisent un monde fondé sur la vitesse, l'immédiateté, la superficialité, le profit et la mort.; Les "technologues" (du grec logos : "discours") : Les nuisances et les dangers sont également repérés, mais l'approche s'apparente à la sociologie compréhensive. La critique ne portant pas tant sur les objets techniques que sur le regard porté sur eux, la "révolution numérique" est analysée à l'aune des motivations humaines profondes telles que la volonté de puissance, la propension au confort matériel maximal ou la tendance à sacraliser son environnement. L'association Technologos (créée en 2012) affiche sur la page d'accueil de son site cette citation de Jacques Ellul : "Ce n'est pas la technique qui nous asservit, mais le sacré transféré à la technique". Y est cultivé le débat contradictoire de façon déconcentrée, depuis des groupes locaux qui, de façon autonome, organisent des séminaires, des débats et des conférences. En septembre 2013, l'association a tenu ses premières assises à l'université de la Sorbonne, à Paris, consacrées au concept d'autonomie de la technique.
Légèrement en marge de ces positionnements axés sur l'analyse de la technique et du phénomène numérique, s'inscrivent des associations où l'on considère que l'évolution actuelle de nos sociétés est essentiellement déterminée par les cheminements de la science et des choix politiques qui en découlent. Cette approche est principalement défendue par l'association Vivagora (créée en 2003), la Fondation Sciences citoyennes (créée en 2006) et l'association Avicenn (Association de Veille et d'Information Civique sur les Enjeux des Nanosciences et des Nanotechnologies) (créée en 2010). Ces formations se donnent pour principaux objectifs de bâtir des expertises et lancer des signaux d'alerte.

Voir aussi

Articles connexes
A: Accès à internet à haut débit; Accessibilité numérique; Aménagement numérique; Apprentissage automatique (machine learning); Apprentissage profond (deep learning); Automation; Automation industrielle; Autonomie de la science.
B: Bibliothèque numérique; Big data; Blogosphère; Bouquet numérique de télédiffusion.
C: Cerveau artificiel; Cloud computing; Communauté virtuelle; Conseil national du numérique; Convergence numérique; Culture numérique.
D: Dématérialisation; Dépendance à internet; Dépendance au jeu vidéo; Déterminisme technologique; Données personnelles.
E: Commerce en ligne; Formation en ligne; Économie numérique; Enseignement assisté par ordinateur; Espace public numérique; Exploration de données.
F: Fab lab; Formation en ligne ouverte à tous; Fracture numérique (générationnelle).
G: Geek; Géolocalisation; Gouvernance d'Internet.
H: Hacklab; Histoire d'Internet; Human Brain Project; Humanités numériques.
I: Identité numérique (Internet); Infobésité; Intelligence artificielle; Interactions homme-machine; Internet des objets.
J: Jeu en ligne.
L: Livre numérique.
M: Marketing électronique; Téléchargement de l'esprit; Monde virtuel.
N: NBIC; Natif numérique; Net-sociologie; Nomadisme numérique; Nouvelles technologies.
O: Web des Objets; Observatoire du numérique.
P: Page web; Photographie numérique; Progrès technique.
Q: Quantified self.
R: Réalité augmentée; Réalité virtuelle; Réseautage social; Réseaux informatiques; Révolution informatique; Radio-identification.
S: Sécurité des systèmes d'information; Simulation informatique; Singularité technologique; Social tech; Social Web; Souveraineté numérique.
T: Tablette tactile; Technologies de l'information et de la communication; Télé-enseignement; Troisième révolution industrielle.
U: Ubérisation.
V: Vie privée et informatique; Village planétaire; Vision par ordinateur; Vote électronique.
W: Web 2.0; Web 3.0.