L'apprentissage automatique (en anglais machine learning, littéralement "l'apprentissage machine") ou apprentissage statistique, champ d'étude de l'intelligence artificielle, concerne la conception, l'analyse, le développement et l'implémentation de méthodes permettant à une machine (au sens large) d'évoluer par un processus systématique, et ainsi de remplir des tâches difficiles ou problématiques par des moyens algorithmiques plus classiques.
L'analyse peut concerner des graphes, arbres, ou courbes (par exemple, la courbe d'évolution temporelle d'une mesure ; on parle alors de données continues, par opposition aux données discrètes associées à des attributs-valeurs classiques) au même titre que de simples nombres. Voir l'article Inférence bayésienne.
Un exemple possible d'apprentissage automatique est celui de la classification : étiqueter chaque donnée en l'associant à une classe. Différents systèmes d'apprentissage existent, listés ci-dessous.

Principes
Les algorithmes utilisés permettent, dans une certaine mesure, à un système piloté par ordinateur (un robot éventuellement), ou assisté par ordinateur, d'adapter ses analyses et ses comportements en réponse, en se fondant sur l'analyse de données empiriques provenant d'une base de données ou de capteurs.
La difficulté réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire (on parle d'explosion combinatoire). On confie donc à des programmes le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que la nature des données d'entrainement n'est pas connue.
Ces programmes, selon leur degré de perfectionnement, intègrent éventuellement des capacités de traitement probabiliste des données, d'analyse de données issues de capteurs, de reconnaissance (reconnaissance vocale, reconnaissance de forme, d'écriture...), de data-mining, d'informatique théorique...

Applications
L'apprentissage automatique est utilisé pour doter des ordinateurs ou des machines de systèmes de : perception de leur environnement : vision, reconnaissance d'objets (visages, schémas, langages naturels, écriture, formes syntaxiques...) ; moteurs de recherche ; aide aux diagnostics, médical notamment, bio-informatique, chémoinformatique ; interfaces cerveau-machine ; détection de fraudes à la carte de crédit, analyse financière, dont analyse du marché boursier ; classification des séquences d'ADN ; jeu ; génie logiciel ; adaptation de sites Web ; locomotion de robots ; analyse prédictive en matière juridique et judiciaire...
Exemples : Un système d'apprentissage automatique peut permettre à un robot ayant la capacité de bouger ses membres mais ne sachant initialement rien de la coordination des mouvements permettant la marche, d'apprendre à marcher. Le robot commencera par effectuer des mouvements aléatoires, puis, en sélectionnant et privilégiant les mouvements lui permettant d'avancer, mettra peu à peu en place une marche de plus en plus efficace ;; La reconnaissance de caractères manuscrits est une tâche complexe car deux caractères similaires ne sont jamais exactement égaux. On peut concevoir un système d'apprentissage automatique qui apprend à reconnaître des caractères en observant des "exemples", c'est-à-dire des caractères connus.

Types d'apprentissage
Les algorithmes d'apprentissage peuvent se catégoriser selon le mode d'apprentissage qu'ils emploient : Apprentissage supervisé : Si les classes sont prédéterminées et les exemples connus, le système apprend à classer selon un modèle de classement ; on parle alors d'apprentissage supervisé (ou d'analyse discriminante). Un expert (ou oracle) doit préalablement étiqueter des exemples. Le processus se passe en deux phases. Lors de la première phase (hors ligne, dite d'apprentissage), il s'agit de déterminer un modèle des données étiquetées. La seconde phase (en ligne, dite de test) consiste à prédire l'étiquette d'une nouvelle donnée, connaissant le modèle préalablement appris. Parfois il est préférable d'associer une donnée non pas à une classe unique, mais une probabilité d'appartenance à chacune des classes prédéterminées (on parle alors d'apprentissage supervisé probabiliste). ex. L'analyse discriminante linéaire ou les SVM en sont des exemples typiques. Autre exemple : en fonction de points communs détectés avec les symptômes d'autres patients connus (les exemples), le système peut catégoriser de nouveaux patients au vu de leurs analyses médicales en risque estimé (probabilité) de développer telle ou telle maladie. Apprentissage non supervisé : Quand le système ou l'opérateur ne disposent que d'exemples, mais non d'étiquettes, et que le nombre de classes et leur nature n'ont pas été prédéterminés, on parle d'apprentissage non supervisé ou clustering. Aucun expert n'est requis. L'algorithme doit découvrir par lui-même la structure plus ou moins cachée des données. Le partitionnement de données, data clustering en anglais, est un algorithme d'apprentissage non supervisé. Le système doit ici - dans l'espace de description (la somme des données) - cibler les données selon leurs attributs disponibles, pour les classer en groupe homogènes d'exemples. La similarité est généralement calculée selon une fonction de distance entre paires d'exemples. C'est ensuite à l'opérateur d'associer ou déduire du sens pour chaque groupe et pour les motifs (patterns en anglais) d'apparition de groupes, ou de groupes de groupes, dans leur "espace". Divers outils mathématiques et logiciels peuvent l'aider. On parle aussi d'analyse des données en régression (ajustement d'un modèle par une procédure de type moindres carrés ou autre optimisation d'une fonction de coût). Si l'approche est probabiliste (c'est-à-dire que chaque exemple, au lieu d'être classé dans une seule classe, est caractérisé par un jeu de probabilités d'appartenance à chacune des classes), on parle alors de "soft clustering" (par opposition au "hard clustering"). Cette méthode est souvent source de sérendipité. ex. Pour un épidémiologiste qui voudrait dans un ensemble assez large de victimes de cancer du foie tenter de faire émerger des hypothèses explicatives, l'ordinateur pourrait différencier différents groupes, que l'épidémiologiste chercherait ensuite à associer à divers facteurs explicatifs, origines géographique, génétique, habitudes ou pratiques de consommation, expositions à divers agents potentiellement ou effectivement toxiques (métaux lourds, toxines telle que l'aflatoxine, etc.). Apprentissage semi-supervisé : Effectué de manière probabiliste ou non, il vise à faire apparaître la distribution sous-jacente des exemples dans leur espace de description. Il est mis en oeuvre quand des données (ou "étiquettes") manquent... Le modèle doit utiliser des exemples non étiquetés pouvant néanmoins renseigner. ex. En médecine, il peut constituer une aide au diagnostic ou au choix des moyens les moins onéreux de tests de diagnostic. Apprentissage partiellement supervisé : probabiliste ou non, quand l'étiquetage des données est partiel. C'est le cas quand un modèle énonce qu'une donnée n'appartient pas à une classe A, mais peut-être à une classe B ou C (A, B et C étant 3 maladies par exemple évoquées dans le cadre d'un diagnostic différentiel). Apprentissage par renforcement : l'algorithme apprend un comportement étant donné une observation. L'action de l'algorithme sur l'environnement produit une valeur de retour qui guide l'algorithme d'apprentissage. ex. L'algorithme de Q-learning est un exemple classique. Apprentissage par transfert : L'apprentissage par transfert peut être vu comme la capacité d'un système à reconnaître et appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes. La question qui se pose est : comment identifier les similitudes entre la ou les tâche(s) cible(s) et la ou les tâche(s) source(s), puis comment transférer la connaissance de la ou des tâche(s) source(s) vers la ou les tâche(s) cible(s).

Algorithmes utilisés
Ce sont dans ce domaine : les machines à vecteur de support ;; le boosting ;; les réseaux de neurones pour un apprentissage supervisé ou non-supervisé ;; la méthode des k plus proches voisins pour un apprentissage supervisé ;; les arbres de décision ;; les méthodes statistiques comme le modèle de mixture gaussienne ;; la régression logistique ;; l'analyse discriminante linéaire ;; les algorithmes génétiques et la programmation génétique.
Ces méthodes sont souvent combinées pour obtenir diverses variantes d'apprentissage. L'utilisation de tel ou tel algorithme dépend fortement de la tâche à résoudre (classification, estimation de valeurs...).
L'apprentissage automatique est utilisé pour un large spectre d'applications, par exemple : moteur de recherche,; aide au diagnostic,; détection de donnée aberrante,; détection de données manquantes,; détection de fraudes,; analyse des marchés financiers,; reconnaissance de la parole,; reconnaissance de l'écriture manuscrite,; analyse et indexation d'images et de vidéo, en particulier pour la recherche d'image par le contenu,; robotique,.

Facteurs de pertinence et d'efficacité
La qualité de l'apprentissage et de l'analyse dépendent du besoin en amont et a priori de la compétence de l'opérateur pour préparer l'analyse. Elle dépend aussi de la complexité du modèle (spécifique ou généraliste), de son adéquation et de son adaptation au sujet à traiter. In fine, la qualité du travail dépendra aussi du mode (de mise en évidence visuelle) des résultats pour l'utilisateur final (un résultat pertinent pourrait être caché dans un schéma trop complexe, ou mal mis en évidence par une représentation graphique inappropriée).
Avant cela, la qualité du travail dépendra de facteurs initiaux contraignants, liées à la base de données : 1) Nombre d'exemples (moins il y en a, plus l'analyse est difficile, mais plus il y en a, plus le besoin de mémoire informatique est élevé et plus longue est l'analyse) ;; 2) Nombre et qualité des attributs décrivant ces exemples. La distance entre deux "exemples" numériques (prix, taille, poids, intensité lumineuse, intensité de bruit, etc) est facile à établir, celle entre deux attributs catégoriels (couleur, beauté, utilité...) est plus délicate ;; 3) Pourcentage de données renseignées et manquantes ;; 4) "Bruit" : le nombre et la "localisation" des valeurs douteuses (erreurs potentielles, valeurs aberrantes...) ou naturellement non-conformes au pattern de distribution générale des "exemples" sur leur espace de distribution impacteront sur la qualité de l'analyse.

Prospective
Il est tentant de s'inspirer des êtres vivants (mais non de les copier naïvement) pour concevoir des machines capables d'apprendre. Les notions de percept et de concept comme phénomènes neuronaux physiques ont d'ailleurs été popularisés dans le monde francophone par Jean-Pierre Changeux.
Même si l'apprentissage automatique reste encore avant tout un sous-domaine de l'informatique, il est étroitement lié opérationnellement aux sciences cognitives, aux neurosciences, à la biologie et à la psychologie, et pourrait à la croisée de ces domaines, nanotechnologies, biotechnologies, informatique et sciences cognitives, aboutir à des systèmes d'intelligence artificielle ayant une assise plus vaste. Deux professeurs actuels du Collège de France y assurent un enseignement public sur le sujet : Stanislas Dehaene orienté sur l'aspect bayésien des neurosciences, et Yann LeCun, pionnier de l'apprentissage profond et depuis 2016 également directeur de Facebook AI Research (FAIR), nouveau centre de recherche européen de Facebook basé à Paris et dédié à l'intelligence artificielle.

Voir aussi

Articles connexes: Algorithme; Algorithme espérance-maximisation; Analyse en composantes principales; Apprentissage profond; Apprentissage supervisé; Carte auto-adaptative; Extraction de connaissances; Intelligence artificielle; Méthode des nuées dynamiques; Partitionnement de données; Régression logistique; Regroupement hiérarchique; Réseau de neurones; Science des données; Sérendipité; Théorème de Cox-Jaynes; Théorie de l'apprentissage statistique.