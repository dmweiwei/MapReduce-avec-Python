Le terme de chambre chinoise désigne une expérience de pensée imaginée par John Searle vers 1980. Searle se demandait si un programme informatique, si complexe soit-il, serait suffisant pour donner un esprit à un système. Cette expérience de pensée vise à montrer qu'une intelligence artificielle ne peut être qu'une intelligence artificielle faible et ne peut que simuler une conscience, plutôt que de posséder des authentiques états mentaux de conscience et d'intentionnalité. Elle vise à montrer également que le test de Turing est insuffisant pour déterminer si une IA possède ou non ces états mentaux.

Le principe
Dans cette expérience de pensée, Searle imagine une personne qui n'a aucune connaissance du chinois (en l'occurrence, lui-même) enfermée dans une chambre. On met à disposition de cette personne un catalogue de règles permettant de répondre à des phrases en chinois. Ces règles sont parfaitement claires pour l'opérateur. Leur application se base uniquement sur la syntaxe des phrases. Une phrase d'une certaine forme syntaxique en chinois est corrélée avec une phrase d'une autre forme syntaxique. L'opérateur enfermé dans la chambre reçoit donc des phrases écrites en chinois et, en appliquant les règles qu'il a à sa disposition, il produit d'autres phrases en chinois qui constituent en fait des réponses à des questions posées par un vrai sinophone situé à l'extérieur de la chambre. Du point de vue du locuteur qui pose les questions, la personne enfermée dans la chambre se comporte comme un individu qui parlerait vraiment chinois. Mais, en l'occurrence, cette dernière n'a aucune compréhension de la signification des phrases en chinois qu'elle transforme. Elle ne fait que suivre des règles prédéterminées.
En poursuivant ironiquement la procédure du test de Turing, test censé démontrer qu'un programme informatique sophistiqué peut être qualifié d'intelligent, Searle imagine que le programme déterminant les réponses qui sont données à l'interlocuteur sinophone devient si sophistiqué, et la personne non sinophone qui répond aux questions devient si habile dans la manipulation des symboles, qu'à la fin de l'expérience, les réponses qu'elle donne aux questions ne peuvent être distinguées de celles que donnerait un vrai locuteur chinois de langue maternelle, bien que selon Searle, la personne qu'on imagine enfermée dans la chambre ne comprenne toujours pas un mot de chinois.
Cette expérience de pensée montre qu'il ne suffit pas d'être capable de reproduire exactement les comportements linguistiques d'un locuteur chinois pour parler chinois, car parler le chinois, ou n'importe quelle autre langue, ce n'est pas juste dire les bonnes choses au bon moment, c'est aussi signifier ou vouloir dire ce qu'on dit : un usage maîtrisé du langage se double ainsi d'une conscience du sens de ce qu'on dit (conscience intentionnelle) et la reproduction artificielle, même parfaite, d'un comportement linguistique ne suffit pas à produire une telle conscience.

La découverte du problème
Searle résume les motivations qui l'ont conduit à concevoir son expérience de pensée de la façon suivante : Je ne connaissais rien (en 1971) à l'intelligence artificielle. J'ai acheté un manuel au hasard, dont la démarche argumentative m'a sidéré par sa faiblesse. Je ne savais pas alors que ce livre allait marquer un tournant dans ma vie. Il expliquait comment un ordinateur pouvait comprendre le langage. L'argument était qu'on pouvait raconter une histoire à un ordinateur et qu'il était capable ensuite de répondre à des questions relatives à cette histoire bien que les réponses ne soient pas expressément données dans le récit. L'histoire était la suivante : un homme va au restaurant, commande un hamburger, on lui sert un hamburger carbonisé, l'homme s'en va sans payer. On demande à l'ordinateur :"A-t-il mangé le hamburger. Il répond par la négative. Les auteurs étaient très contents de ce résultat, qui était censé prouver que l'ordinateur possédait les mêmes capacités de compréhension que nous. C'est à ce moment-là que j'ai conçu l'argument de la chambre chinoise.
Un argument contre les conceptions fonctionnalistes de l'esprit
Searle oppose son expérience de pensée d'abord aux défenseurs de la thèse de l'intelligence artificielle "forte", thèse défendue la première fois dans les années cinquante par Alan Turing (cf. test de Turing) et consistant à reconnaître l'existence d'un esprit chez un interlocuteur (personne ou machine) sur la seule base de son comportement linguistique.
Plus largement, Searle s'oppose aux conceptions fonctionnalistes de l'esprit qui le définissent en termes de fonctions biologiques ou physiques sans tenir compte des aspects subjectifs de l'expérience, c'est-à-dire en se refusant à expliquer récursivement l'esprit... par l'esprit.
Ces fonctions sont conçues intentionnellement par les fonctionnalistes (notamment chez H. Putnam et J. Fodor) en faisant totalement abstraction du support physique sur lequel elles se réalisent. Dès lors, de même qu'il est possible de reproduire artificiellement les fonctions d'un coeur, il serait possible de reproduire artificiellement les fonctions intellectuelles et sensorielles du cerveau à l'aide de n'importe quel support adapté. C'est également le point d'une thèse soutenue en 1998 à Lille devant un jury comprenant Jean-Paul Delahaye, Calculabilité, physique et cognition
Dans la version computationnaliste du fonctionnalisme, une production de pensée est même parfaitement envisageable avec un programme informatique approprié. Or, l'expérience de la chambre chinoise montre qu'on peut imaginer un système automatique par définition sans esprit et pourtant indiscernable, d'un point de vue fonctionnel, d'un être humain possédant une intentionnalité. La reproduction artificielle d'un comportement qu'on pourrait décrire comme intentionnel ne suffirait donc pas à produire un esprit, c'est-à-dire, une conscience intentionnelle.

Les objections
L'objection la plus fréquemment avancée à l'encontre de l'argument de la chambre chinoise est celle que Searle a nommée, par anticipation, "la réponse du système". Selon celle-ci, le système dont fait partie la personne qui suit les instructions du manuel comprend bel et bien le chinois, en dépit du fait que la personne elle-même ne comprend pas cette langue. Dans le système que constitue la chambre chinoise, la personne joue alors le rôle de l'unité centrale (ou processeur) d'un ordinateur. Mais le processeur n'est que l'une des nombreuses composantes d'un ordinateur. Dans le cas d'un ordinateur suffisamment sophistiqué pour penser, ce n'est pas le processeur pris isolément qui pense mais plutôt l'ensemble du système dont il fait partie, car c'est le système tout entier qui permet de fournir les réponses appropriées.
Pour Searle, cette objection n'est pas recevable car elle implique l'idée selon lui absurde qu'il y aurait une conscience de la chambre chinoise qui n'existerait pas au niveau de la personne qui fournit les réponses, alors même qu'il est présupposé que cette personne est le seul être conscient de cette chambre. Cependant on peut remarquer que le manuel est, lui, le produit d'une conscience. Il a forcement été écrit par quelqu'un qui comprend la langue chinoise.
Zenon Pylyshyn (en) souligne pour sa part la vacuité de la notion d'intentionnalité, ou de "pouvoirs causaux" du cerveau, mis en avant par Searle pour différencier une authentique compréhension, de l'apparente compréhension d'une chambre chinoise. Pour illustrer ce problème, il imagine de remplacer, petit à petit, dans un cerveau humain, les cellules cérébrales par des équivalents électroniques ayant exactement les mêmes propriétés. La personne continuerait à discuter et à posséder la même compréhension apparente, mais selon les conceptions de Searle, perdrait petit à petit la faculté de véritable compréhension. Mais la position de Searle n'explique pas clairement à quel moment, pourquoi et en quoi la faculté de compréhension de la personne a changé.
Une autre objection, développée en particulier par Douglas Hofstadter, vient de la linguistique (par exemple de la sémiologie, ou de l'étude de la fonction perlocutoire) et affirme que l'expérience de pensée de Searle est en fait impossible, car on ne peut produire des réponses adaptées à l'aide de seules règles syntaxiques ; une connaissance du monde est nécessaire pour, par exemple en anglais, distinguer correctement les fonctions grammaticales dans le célèbre exemple Time flies like an arrow; fruit flies like a banana (en).
On peut enfin remarquer (cette analyse est développée par exemple par Zach Weiner) qu'il y a là une pétition de principe : personne ne sait écrire une telle liste de règles syntaxiques (sinon, on aurait déjà réussi le test de Turing), et on ne sait même pas si cette liste existe en principe, du moins si on exige qu'elle ait un volume réaliste.