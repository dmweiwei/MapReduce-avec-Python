Le traitement automatique du langage naturel ou de la langue naturelle (abr. TALN) ou des langues (abr. TAL) est une discipline à la frontière de la linguistique, de l'informatique et de l'intelligence artificielle, qui concerne l'application de programmes et techniques informatiques à tous les aspects du langage humain.
Ainsi, le TAL ou TALN est parfois nommé ingénierie linguistique.

Histoire
L'histoire du TAL commence dans les années 1950, bien que l'on puisse trouver des travaux antérieurs.
En 1950, Alan Turing éditait un article célèbre sous le titre "Computing machinery and intelligence" qui propose ce qu'on appelle à présent le test de Turing comme critère d'intelligence. Ce critère dépend de la capacité d'un programme informatique de personnifier un humain dans une conversation écrite en temps réel, de façon suffisamment convaincante que l'interlocuteur humain ne peut distinguer sûrement - sur la base du seul contenu de la conversation - s'il interagit avec un programme ou avec un autre vrai humain.
L'expérience de Georgetown en 1954 comportait la traduction complètement automatique de plus de soixante phrases russes en anglais. Les auteurs prétendaient que dans un délai de trois ou cinq ans, la traduction automatique ne serait plus un problème.
Pendant les années 1960, SHRDLU, un système de langage naturel appelé "blocks world" dont la base était des vocabulaires relativement restreints, fonctionnait extrêmement bien, invitant les chercheurs à l'optimisme.
Cependant, le progrès réel était beaucoup plus lent, et après le rapport ALPAC (en) de 1966, qui constatait qu'en dix ans de recherches les buts n'avaient pas été atteints, l'ambition s'est considérablement réduite.
ELIZA était une simulation à la manière de la psychothérapie rogérienne, écrite par Joseph Weizenbaum entre 1964 à 1966. N'employant presque aucune information sur la pensée ou l'émotion humaine, ELIZA parvenait parfois à offrir un semblant stupéfiant d'interaction humaine. Quand le "patient" dépassait la base de connaissances (par ailleurs très petite), ELIZA pouvait fournir une réponse générique, par exemple, en réponse à "J'ai mal à la tête" dire "Comment cela se manifeste-t-il ".
Pendant les années 1970 beaucoup de programmeurs ont commencé à écrire des "ontologies conceptuelles", dont le but était de structurer l'information en données compréhensibles par l'ordinateur. C'est le cas de MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), SCRUPULE (Lehnert, 1977), Politics (Carbonell, 1979), Plot Units (Lehnert 1981).
Pendant ce temps, beaucoup de chatterbots à la manière d'ELIZA ont été écrits comme PARADE, Racter, et Jabberwacky. Dès les années 1980, à mesure que la puissance informatique augmentait et devenait moins chère, les modèles statistiques pour la traduction automatique ont reçu de plus en plus d'intérêt.

TAL statistique
Les utilisations statistiques du traitement du langage naturel reposent sur des méthodes stochastiques, probabilistes ou simplement statistiques pour résoudre certaines difficultés discutées ci-dessus, particulièrement celles qui surviennent du fait que les phrases très longues sont fortement ambiguës une fois traitées avec des grammaires réalistes, autorisant des milliers ou des millions d'analyses possibles. Les méthodes de désambiguïsation comportent souvent l'utilisation de corpus et d'outils de formalisation comme les modèles de Markov. Le TAL statistique comporte toutes les approches quantitatives du traitement linguistique automatisé, y compris la modélisation, la théorie de l'information, et l'algèbre linéaire. La technologie pour TAL statistique vient principalement de l'apprentissage automatique et le data mining, tous deux en tant qu'ils impliquent l'apprentissage à partir des données venant de l'intelligence artificielle.

Champs de recherche et applications
Le champ du traitement automatique du langage couvre de très nombreuses disciplines de recherche qui peuvent mettre en oeuvre des compétences aussi diverses que les mathématiques appliquées ou le traitement du signal.

Syntaxe: Lemmatisation: Regroupement des mots d'une même famille dans un texte, afin de réduire ces mots à leur forme canonique (le lemme), comme petit, petite, petits, et petites. Certaines conjugaisons peuvent rendre cette tâche complexe pour des ordinateurs, comme retrouver la forme canonique "avoir" depuis "eussions eu". En revanche, "des avions" et "nous avions" n'ont pas le même lemme. Morphologie: Regroupement de différents mots à travers leur parties, comme les suffixes, préfixes, radicaux. Par exemple, enneigement peut se décomposer en "en- + neige + -ment". Étiquetage morpho-syntaxique: Assigne chaque mot d'un texte à sa catégorie grammaticale. Par exemple, le mot ferme peut être un verbe dans "il ferme la porte" et un nom dans "il va à la ferme". Analyse syntaxique: Étiquetage morpho-syntaxique de chacun des mots d'un texte, comme dans un arbre syntaxique. Certaines phrases ambiguës peuvent être interprétées de plusieurs manières différentes, comme "je regarde l'homme avec les jumelles", qui peut signifier "je regarde l'homme en utilisant des jumelles", ou "je regarde l'homme qui a des jumelles", ou "je regarde l'homme qui est accompagné de soeurs jumelles". Délimitation de la phrase: Séparation des phrases d'un texte. À l'écrit, la ponctuation ou la casse permet en général de séparer les phrases, mais des complications peuvent être causées par les abréviations utilisant un point, ou les citations incluant des ponctuations à l'intérieur d'une phrase, etc. Racinisation: Regroupement des mots ayant une racine commune et appartenant au même champ lexical. Par exemple, pêche, pêcher, pêcheur ont la même racine, mais ni la pêche (le fruit), ni le Péché, ne font partie du même champ lexical. Séparation des mots: Dans la langue parlée, les phrases ne sont qu'une chaîne de phonèmes, où l'espace typographique n'est pas prononcé. Par exemple, la phrase -bn-apatmo- peut être comprise identiquement comme "un bon appartement chaud" et "un Bonaparte manchot".
Sémantique: Traduction automatique: Il s'agit de l'un des problèmes les plus complexes, dit IA-complet, qui nécessite de nombreuses connaissances, non seulement linguistiques, mais aussi concernant le monde. Il s'agit de la première application de recherche, active dès les années 1950. Génération automatique de textes: Écriture de textes syntaxiquement et sémantiquement corrects, pour produire par exemple des bulletins météo ou des rapports automatisés. Résumé automatique de texte, reformulation et paraphrasage: Extraction du contenu pertinent d'une texte, détection des informations les plus importantes, des redondances, afin de générer un texte cohérent humainement crédible. Désambiguïsation lexicale: Problème encore non résolu, consistant à déterminer le sens d'un mot dans une phrase, lorsqu'il peut avoir plusieurs sens possibles, selon le contexte général. Correction orthographique: outre une comparaison aux mots du dictionnaire et une recherche approximative afin de proposer des corrections, il existe les correcteurs grammaticaux qui utilisent la sémantique et le contexte afin de corriger les homophonies. Agents conversationnels, et systèmes de questions-réponses: Combinaison d'une étape de compréhension du langage puis une étape de génération de texte. Détection de coréférences et résolution d'anaphores: Détection de la liaison entre plusieurs mots d'une phrase faisant référence à un même sujet.
Traitement du signal (parole et graphie): Reconnaissance de l'écriture manuscrite, reconnaissance optique de caractères et lecture automatique de document: Système d'analyse et de traitement des images, couplés à des règles linguistiques permettant d'évaluer la probabilité d'apparition des lettres et mots décodés. Reconnaissance automatique de la parole: Analyse acoustique, association entre segments élémentaires sonore et des éléments lexicaux, puis correspondance des motifs obtenus avec des mots courant, ou des suites de mots apparaissant fréquemment. Synthèse vocale: Une translation vers l'alphabet phonétique est la plus souvent utilisée, mais la catégorie grammaticale est aussi à prendre en compte; par exemple, il faut reconnaître le second -ent comme muet dans l'exemple "Les présidents président". Les mots dont la prononciation est irrégulière doivent être stockés. De plus, l'intonation et la prosodie sont également à prendre en compte afin d'obtenir un effet naturel. Traitement de la parole: Regroupe les deux catégories ci-dessus. La détection des langues et des dialectes: Tant à partir des textes ou des énoncés parlés.
Extraction d'informations: Fouille de textes: Recherche d'informations spécifiques dans un corpus de documents donnés, qui utilise l'indexation de contenu. Recherche d'information: Sous-domaine de la fouille de texte ; l'application la plus connue est les moteurs de recherche, qui passent également l'analyse des méta-données et des liens entre les pages elles-mêmes. Reconnaissance d'entités nommées: Détermination dans un texte des noms propres, tels que des personnes ou des endroits, ainsi que les quantités, valeurs, ou dates. Classification et catégorisation de documents: Systèmes de tutorat intelligents; Utilisés notamment pour l'enseignement des langues; Analyse de sentiment: Vise à extraire le ressenti d'un texte (généralement positif ou négatif) en fonction des mots et du type de langage utilisé, d'indices typographiques ou de la personne qui l'a écrit. La recommandation automatique de documents: Consistant à extraire l'information importante d'une base de documents afin de les relier en "séries", afin de proposer ses éléments aux personnes intéressées par d'autres éléments de cette série.
Bibliométrie
La bibliométrie est l'utilisation du traitement automatique des langues sur des publications scientifiques.

Étude bibliométrique du TAL
La première étude d'envergure a été réalisée en 2013 à l'occasion de l'anniversaire de l'Association for Computational Linguistics (en) (ACL) avec un atelier intitulé "Rediscovering 50 Years of Discoveries in Natural Language Processing".
La même année a eu lieu l'action NLP4NLP, opération de bibliométrie d'application des outils de TAL aux archives du TAL depuis les années soixante jusqu'à nos jours par Joseph Mariani, Gil Francopoulo et Patrick Paroubek. Il s'agit par exemple de déterminer automatiquement quels sont les inventeurs des termes techniques que nous utilisons actuellement. Un autre champ d'étude est de déterminer quels sont les copier-coller éventuels que les chercheurs du TAL effectuent quand ils écrivent un article scientifique.

Voir aussi

Articles connexes: Linguistique informatique; Lexical markup framework (LMF), travaux de normalisation ISO des lexiques du TAL; Modular Audio Recognition Framework (MARF); ATALA Association pour le traitement automatique des langues : Société savante de référence pour la francophonie.; Conférences TALN; LREC; LRE Map, base de données des ressources utilisées dans le TAL.