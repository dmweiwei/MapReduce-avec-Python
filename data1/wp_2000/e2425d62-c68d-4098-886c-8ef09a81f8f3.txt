Une carte graphique ou carte vidéo (anciennement, par abus de langage, une carte VGA), ou encore un adaptateur graphique, est une carte d'extension d'ordinateur dont le rôle est de produire une image affichable sur un écran.
La carte graphique envoie à l'écran des images stockées dans sa propre mémoire, à une fréquence et dans un format qui dépendent d'une part de l'écran branché et du port sur lequel il est branché (grâce au Plug and Play) et de sa configuration interne d'autre part.

Historique

Cartes graphiques 2D-3D
Les premières cartes graphiques ne permettaient, au début de l'ère informatique, qu'un affichage en 2D et se connectaient sur un port Industry standard architecture (ISA) 8 bits ; ce sont les cartes Monochrome Display Adapter (MDA). Bien que dénommées "cartes graphiques", elles n'affichaient, en monochrome, que de simples caractères codés sur 8 bits, dont une partie était réservée au graphisme ; c'est l'adressage direct en mode ASCII (mode encore utilisé au démarrage par le BIOS de la plupart des ordinateurs en 2009).
Les premières cartes graphiques pouvant adresser un point individuel de l'affichage n'apparaissent qu'en 1981 pour le grand public, avec les cartes CGA, (Color Graphic Adapter), qui permettaient un adressage de points dans une résolution de 320 colonnes sur 200 lignes en 4 couleurs différentes. Suivent alors une succession de cartes dédiées au graphisme sur ordinateur poussant de plus en plus loin le nombre de lignes et de colonnes adressables, ainsi que le nombre de couleurs simultanées pouvant être affichées ; ce sont les modes graphiques utilisables. De plus en plus de fonctions assurées par le processeur sont petit à petit gérées par le contrôleur graphique des cartes, comme le tracé de lignes, de surfaces pleines, de cercles, etc. fonctions très utiles pour accompagner la naissance des systèmes d'exploitation basés sur des interfaces graphiques et en accélérer l'affichage.
Avec l'évolution des techniques, le port ISA est remplacé par le port PCI pour augmenter la vitesse de transfert entre le processeur et la carte graphique. En plus des cartes graphiques d'affichage en 2D, apparaissent dans les années 1990 des cartes dédiées à la gestion et l'affichage d'éléments représentés en 3D, comme les cartes 3DFX. Puis apparurent les cartes graphiques 2D-3D ayant l'avantage de n'occuper qu'un seul connecteur AGP ou PCI au lieu de deux (avant 1998). En effet, jusqu'alors, les cartes 2D étaient proposées séparément des cartes dites accélératrice 3D, chacune ayant un processeur graphique spécifique. Depuis la sortie des premières cartes 2D-3D intégrées par ATI en 1996, toutes les cartes graphiques modernes gèrent la 2D et la 3D au sein d'un seul circuit intégré.

Fonction
Exemple de carte graphique NVIDIA Quadro destinée à des professionnels.
Depuis la fin des années 1995, les cartes graphiques ont fortement évolué. Autrefois, la fonction essentielle d'une carte graphique était de transmettre les images produites par l'ordinateur à l'écran. C'est encore sa fonction principale sur beaucoup de machines à vocation bureautique où l'affichage d'images en 3D n'offre que peu d'intérêt. Toutefois aujourd'hui même les cartes graphiques les plus simples gèrent aussi le rendu d'images en 3D. C'est une activité très coûteuse en termes de calculs et en termes de bande passante mémoire. Le GPU (pour Graphical Processing Unit) est donc devenu un composant très complexe, très spécialisé et presque imbattable dans sa catégorie (rendu d'images en 3 dimensions). Hormis pour les jeux vidéo ou quelques usages en infographie, les possibilités des cartes graphiques ne sont que très peu exploitées en pratique. Ainsi, ce sont essentiellement les joueurs qui achètent et utilisent des GPU de plus en plus puissants.
Depuis les années 2000, la puissance de calcul des cartes graphiques est devenue tellement importante pour un coût finalement très réduit (100 à 700 pour les modèles grand public) que les scientifiques sont de plus en plus nombreux à vouloir en exploiter le potentiel dans d'autres domaines. Il peut s'agir de faire tourner des simulations de modèles météo, financiers ou toute opération parallélisable et nécessitant une très grande quantité de calcul. NVIDIA et ATI-AMD, les 2 principaux fabricants de cartes graphiques haute performance grand public proposent chacun des solutions propriétaires afin de pouvoir utiliser leur produit pour du calcul scientifique ; pour NVIDIA, on pourra se référer au projet CUDA et pour AMD au projet ATI Stream. On parle à ce titre de General-Purpose Processing on Graphics Processing Units (ou GPGPU).
Dès 1996, les cartes graphiques commencent à intégrer des fonctions de décompression vidéo, comme pour la Rage-Pro du fabricant ATI qui intègre déjà en 1996 certaines fonctions de décompression des flux MPEG2. Sous des appellations variées, se sont depuis développées des technologies qui permettent de soulager le processeur de la charge incombant à la décompression d'une image 25 (PAL-SECAM) ou 30 (NTSC) fois par seconde dans des définitions toujours plus élevées. La prise en charge partielle, ou totale, par les GPU des flux vidéos permet le visionnage de films en haute définition sur des plateformes matérielles aux ressources processeur relativement modestes ; ce qui serait impossible sans eux au regard du nombre d'informations à traiter presque simultanément.
De nos jours, des modèles mobiles ont été créés pour être embarqué dans les ordinateurs portables. Ce sont des cartes souvent dérivées de leurs équivalents de bureaux avec des unités en moins, une fréquence inférieure, etc.
Les cartes graphiques modernes font parfois office de cartes son grâce à leurs sorties son intégrées aux sorties vidéos classiques, comme pour le HDMI. Les pilotes peuvent alors être adapté pour gérer le son, via l'onglet audio des pilotes catalyst par exemple. Cette nouvelle fonction s'est développé grâce à l'ajout de haut-parleurs sur les écrans.

Composants

Processeur graphique
Le processeur graphique (GPU pour Graphical Processing Unit, ou encore VPU pour Visual Processing Unit en anglais) sert à libérer le micro-processeur de la carte mère en prenant en charge les calculs spécifiques à l'affichage et la coordination de graphismes 3D ou la conversion d'espaces colorimétriques YCbCr vers RGB ; quand ce ne sont pas des fonctions vectorielles permettant la reconstruction d'images compressées de certains flux vidéos comme le H.264. Cette division des tâches entre les deux processeurs libère le processeur central de l'ordinateur et en augmente d'autant la puissance apparente.
Le processeur graphique est très souvent muni de son propre radiateur ou ventilateur pour évacuer la chaleur qu'il produit.

Mémoire vidéo
La mémoire vidéo conserve les données numériques qui doivent être converties en images par le processeur graphique et les images traitées par le processeur graphique avant leur affichage. Toutes les cartes graphiques supportent deux méthodes d'accès à leur mémoire. L'une est utilisée pour recevoir des informations en provenance du reste du système, l'autre est sollicitée pour l'affichage à l'écran. La première méthode est un accès direct conventionnel (RAM) comme pour les mémoires centrales, la deuxième méthode est généralement un accès séquentiel à la zone de mémoire contenant l'information à afficher à l'écran.

RAMDAC
Le RAMDAC (Random Access Memory Digital-to-Analog Converter) convertit les images stockées dans la mémoire vidéo en signaux analogiques à envoyer à l'écran de l'ordinateur. Il est devenu inutile avec les sorties DVI (numériques).

BIOS vidéo
Le BIOS vidéo est à la carte graphique ce que le BIOS est à la carte mère. C'est un petit programme enregistré dans une mémoire morte (ROM, pour Read Only Memory) qui contient certaines informations sur la carte graphique (par exemple, les modes graphiques supportés par la carte) et qui sert au démarrage de la carte graphique.

Connexion à la carte mère
La connexion à la carte mère se fait à l'aide d'un port greffé sur un bus. Au cours des années, plusieurs technologies se sont succédé pour satisfaire les besoins de vitesse de transfert sans cesse croissants des cartes graphiques : la première technologie utilisée fut la technologie ISA, utilisée à partir de 1984 pour adjoindre des cartes disposant de plus de mémoire vidéo que les cartes standard fournies par les manufacturiers d'ordinateurs ou des cartes utilisant des jeux d'instructions destinés à accélérer l'affichage des fenêtres ;; certains PC à base de 80486 ont utilisé le bus VLB (Vesa Local Bus), mais ce type de bus fut rapidement abandonné en raison de sa trop grande spécificité ;; avec l'arrivée des premiers processeurs Pentium en 1994, on utilise ensuite l'interface PCI ;; le bus AGP apparu en mai 1997 est actuellement supplanté par le bus PCI Express, apparu en 2004 ;; le PCI Express qui permet d'atteindre le débit de données bi-directionnelles pour le PCI-Express 2.0 (500 Mo-s) est destiné à remplacer tous les connecteurs d'extension internes d'un PC, dont le PCI et l'AGP ;; le bus USB, de nouvelles cartes graphiques externes sont commercialisées, qui profitent du haut débit qu'offre le bus USB dans sa version 2; elles n'arriveront à pleine maturité qu'avec l'USB version 3, permettant d'afficher un nombre d'images par seconde suffisant pour permettre l'affichage de vidéos en mode plein écran.
D'autres types de connexions existent dans d'autres architectures d'ordinateurs, on pourra citer par exemple le bus VME ; mais ce sont des technologies peu répandues et réservées au monde de l'informatique professionnelle et de l'industrie.

Connectique

Interfaces analogiques: L'interface VGA standard : les cartes graphiques sont la plupart du temps équipées d'un connecteur VGA 15 broches (Mini Sub-D, composé de 3 séries de 5 broches), généralement de couleur bleue, permettant notamment la connexion d'un écran CRT ou LCD. Ce type d'interface permet d'envoyer à l'écran 5 signaux analogiques correspondant aux composantes rouges, vertes et bleues de l'image ainsi que les signaux de synchronisation horizontaux et verticaux (R G B H V).; L'interface Vidéo composite : pour la sortie sur un simple téléviseur ou un magnétoscope.; L'interface S-Vidéo : de nombreuses cartes étaient équipées d'une prise S-Vidéo permettant d'afficher ce signal sur une télévision ou un vidéo projecteur compatible.
Les interfaces analogiques tendent progressivement à disparaitre au bénéfice des interfaces numériques (mars 2011). Bien que l'on trouve encore les signaux analogiques présents sur certaines broches des interfaces DVI, ce qui permet la transformation d'un connecteur DVI en connecteur VGA par un simple adaptateur passif : cette adaptation n'est plus possible avec les interfaces purement digitales comme les interfaces HDMI sans composants actifs.

Interfaces numériques
L'interface DVI (Digital Video Interface), présente sur certaines cartes graphiques, permet d'envoyer, aux écrans le supportant, des données numériques. Ceci permet d'éviter des conversions numérique-analogique, puis analogique-numérique, inutiles. Une interface HDMI permettant de relier la carte à un écran haute définition en transmettant également la partie audio (polyvalent, ce format est le remplaçant de la péritel). Le signal est un signal purement numérique. Une interface DisplayPort, une interconnexion digitale audio-vidéo de nouvelle génération, sans droit ni licence, est la seule interface à pouvoir garantir un flux 4K à 60 Hz nativement. À noter que le DVI et le HDMI peuvent supporter les DRM.
Les modèles actuels associent généralement deux types d'interfaces : une interface pour la télévision (S-Vidéo ou HDMI) avec une interface pour écran d'ordinateur (VGA ou DVI). Dans le cas des interfaces analogiques, certaines lignes des signaux servent à transmettre des informations concernant certaines données spécifiques à l'écran utilisé. L'écran peut transmettre des informations comme la définition optimale et ses taux limites de rafraichissement. Cela permet de renseigner intelligemment le système d'exploitation sur la meilleure définition à afficher par exemple (voir DDC pour les informations concernant les informations transmises). Dans le cas des interfaces numériques, des informations sont échangées entre l'écran et la carte graphique afin d'assurer les mêmes fonctions qu'en analogique; avec eux, transitent par la même occasion certaines informations concernant des fonctionnalités supplémentaires, de protection anti-copies par exemple, ou les capacités de transport de son au format numérique.

Détermination de la quantité de mémoire vidéo
La quantité de mémoire vidéo nécessaire pour stocker l'image à afficher dépend de la définition choisie pour l'affichage. Le nombre de couleurs est fonction du nombre de bits utilisés pour le codage de la couleur.
La quantité de mémoire nécessaire est simplement le nombre de pixels utiles multiplié par le nombre de bits pour la couleur par pixel. On divise le tout par huit pour passer en octets (1 octet - 8 bits)
Cette indication est maintenant de peu d'intérêt car la mémoire vidéo d'une carte graphique est utilisée à de nombreuses fins. Elle permet entre autres de fluidifier l'affichage des vidéos ou encore de stocker les informations nécessaires à la synthèse d'images en 3D. Les systèmes d'exploitation modernes comme Windows Vista, Windows 7, Mac OS ou GNU-Linux requièrent tous trois une grande quantité de mémoire vidéo pour optimiser leur affichage. Quant aux jeux vidéo les plus récents, ils fonctionnent d'autant mieux que la quantité de mémoire vidéo est importante. En 2016, on trouve couramment des cartes graphiques équipées de 4 Gio de mémoire.

Comparaison des cartes graphiques non intégrées compatibles PC fixes

Acteurs
L'histoire des cartes graphiques ne se limite pas au seul duel ATI vs NVIDIA. D'autres acteurs ont connu leur heure de gloire ; parmi eux peuvent être cités : S3 Graphics, Trident, Cirrus Logic et 3dfx pour leur série Voodoo 3000 et 4000 et bien sûr SGI qui a fabriqué jusqu'en 2004 ses propres solutions graphiques dédiées au monde professionnel. Sans oublier Intel qui, bien que perdant du terrain, livrait encore en avril 2010 la majorité des solutions graphiques pour PC dans le monde sous la forme de chipsets avec contrôleur graphique intégré. En 2012, Intel avec sa gamme ivy bridge, lance le circuit graphique intégré HD4000.

Voir aussi

Articles connexes: Video Graphics Array (VGA); Enhanced Graphics Adapter (EGA); Color Graphics Adapter (CGA); Monochrome Display Adapter (MDA); Fonctionnement des cartes VGA; Liste des constructeurs de cartes graphiques.