La bulle de filtres, ou bulle de filtrage, est un concept développé par le militant d'Internet Eli Pariser. Si on en croit Eli Pariser, la bulle de filtres désigne l'état dans lequel se trouve un internaute lorsque les informations qu'il recherche sur Internet sont le résultat d'une personnalisation mise en place à son insu. La théorie très médiatisée affirme qu'à partir des différentes données collectées sur l'internaute, des algorithmes vont silencieusement sélectionner les contenus qui seront visibles ou non par lui. Le terme de "bulle de filtres" renverrait à l'isolement produit par ce mécanisme : chaque internaute accéderait à une version différente du web, il resterait dans une "bulle" unique et optimisée pour lui.
Toujours selon Eli Pariser, ce phénomène se rencontre notamment sur les réseaux sociaux et les moteurs de recherche. Des sites tels que Google, Facebook ou Yahoo! n'affichent pas toutes les informations, mais seulement celles sélectionnées pour l'utilisateur. À partir de différentes données (historique, clics, interactions sociales) ces sites prédisent ce qui sera le plus pertinent pour un internaute donné. Ils lui fournissent ensuite l'information la plus pertinente, en omettant celle qui l'est moins selon eux. Si les algorithmes considèrent qu'une information n'est pas pertinente pour un internaute, elle ne lui sera pas présentée.

Effets psycho-sociaux supposés
Selon Pariser, ce phénomène tendrait à reproduire les opinions, croyances et perspectives de l'utilisateur en formant un cercle vicieux. Un internaute d'une orientation politique donnée verrait plus de contenus favorables à cette orientation. Il serait moins soumis à des points de vue contradictoires car les algorithmes sélectionneraient pour lui les contenus les plus pertinents, ceux qui lui plaisent le plus. Par exemple, un internaute qui serait identifié comme "de gauche" par le site, se verrait alors proposé moins de contenus "de droite".
Des requêtes similaires peuvent alors donner des résultats très différents. Supposons par exemple que deux personnes, une plutôt à droite politiquement et l'autre plutôt à gauche, recherchent le terme "BP". Les utilisateurs "de droite" trouveront des informations sur les investissements dans la British Petroleum. Les utilisateurs "de gauche" obtiendront des informations sur la marée noire dans le golfe du Mexique.
La bulle de filtres pourrait influencer les relations sociales. Sur certains réseaux sociaux, la personnalisation algorithmique masquerait les messages les moins pertinents, ceux qui seraient les moins cliqués par l'utilisateur. Moins on interagit avec un "ami" Facebook, moins les messages qu'il publie nous seront visibles, moins l'on sera susceptibles d'interagir avec lui. Pariser met en avant la "disparition" des messages de ses amis conservateurs de son flux d'activité Facebook. Alors qu'il avait ajouté des "amis" conservateurs pour lire leur opinion, la personnalisation ne lui suggérait plus les publications venant de ces personnes. Selon l'algorithme, cela n'était pas pertinent pour Pariser : il n'allait pas cliquer ou lire ces opinions.
Dans un long article, longuement commenté depuis, Katharine Viner, rédactrice en chef du journal The Guardian, estime que le numérique a considérablement ébranlé notre rapport aux faits et que les réseaux sociaux, en particulier, sont grandement responsables de la victoire des partisans (populistes) du référendum sur l'appartenance du Royaume-Uni à l'Union européenne, bien qu'ils aient eux-mêmes reconnu que leurs arguments étaient mensongers. Viner affirme que si les réseaux sociaux colportent volontiers des rumeurs et des "mensonges avérés", cela tient aux bulles de filtres qui, en fonction des pages consultées, renvoient les utilisateurs à ce qu'ils ont l'habitude de consulter et qui, par conséquent tendent à les conforter dans leurs opinions au lieu de stimuler leur esprit critique

Solutions et initiatives
Face à cette situation plusieurs solutions existent ou sont envisageables. Chaque individu peut volontairement sortir de sa bulle de filtres en allant chercher les lecteurs ou médias du bord opposé.
Facebook a reconnu l'existence de ce concept, et commencé à travailler sur des mesures pour en limiter l'impact.
Des sites spécialisés se sont également montés, principalement aux Etats-Unis, permettant à chacun de prendre conscience de ce phénomène, comme allsides.com ou hifromtheotherside.com. Certains moteurs de recherche non personnalisés ont également fait valoir que leur absence de tri idéologique des résultats de recherche permettait de lutter contre les bulles de filtres. Enfin, certaines extensions ont été développées pour les navigateurs web Google Chrome ou Mozilla Firefox permettant de mettre en lumière les bulles de filtres, et contextualiser certaines informations.
En France, le journal Le Drenche propose pour tous les sujets deux avis différents et opposés, avec l'objectif affiché de lutter contre les bulles de filtres.

Au-delà du concept, une lecture plus critique
Malgré la très forte médiatisation de la théorie de la bulle de filtres d'Eli Pariser, qui se veut participer de l'esprit critique, d'autres lectures sont possibles. C'est notamment la thèse d'André Gunthert dans Et si on arrêtait avec les bulles de filtre (André Gunthert est maître de conférence à l'École des hautes études en sciences sociales (EHESS), où il occupe la chaire d'histoire visuelle). D'après lui, "le système de sélection de Facebook ne modifie que de 1 % l'exposition aux contenus politiques de camps opposés". Et donne la parole au spécialiste Dominique Cardon, qui résume: "La bulle, c'est nous qui la créons. Par un mécanisme typique de reproduction sociale. Le vrai filtre, c'est le choix de nos amis, plus que l'algorithme de Facebook."

Bibliographie soutenant la théorie d'Eli Pariser: Eli Pariser, The Filter Bubble: What the Internet Is Hiding from You, Penguin Press (New York, mai 2011).; Eli Pariser, TED Talk. 11 février 2011. "Enregistrement de la conférence".

Bibliographie critique: André Gunthert, Et si on arrêtait avec les bulles de filtre, 13 novembre 2016.; Alexis Delcambre, Alexandre Piquard, "Facebook, faux ami de la démocratie", Le Monde, 3 novembre 2016, p. 14-15.; Gilbert Simondon, Du mode d'existence des objets techniques, Paris, Aubier, 1958.; Aude Lancelin, Le Monde libre, Paris, LLL, 2016.; Eytan Bakshy, Solomon Messing, Lada Adamic,"Exposure to ideologically diverse news and opinion on Facebook, Science, 7 mai 2015.

Liens internes: Algorithme; Chambre d'écho; Ère post-vérité; Réseaux sociaux; Eli Pariser; André Gunthert.