Un centre de données (en anglais data center) est un site physique sur lequel se trouvent regroupés des équipements constituants du système d'information de l'entreprise (ordinateurs centraux, serveurs, baies de stockage, équipements réseaux et de télécommunications, etc.). Il peut être interne et-ou externe à l'entreprise, exploité ou non avec le soutien de prestataires.
C'est un service généralement utilisé pour remplir une mission critique relative à l'informatique et à la télématique. Il comprend en général un contrôle sur l'environnement (climatisation, système de prévention contre l'incendie, etc.), une alimentation d'urgence et redondante, ainsi qu'une sécurité physique élevée.
Des enjeux environnementaux sont liés à la consommation d'électricité des centres de données, et à leur coproduit qu'est la chaleur, dissipée par les serveurs et les systèmes de stockage en particulier.

Description
Tablettes d'équipement de télécommunications. Centre de données de l'Agence Reuters à Londres.
Un centre de données se présente comme un lieu où se trouvent différents équipements électroniques, des ordinateurs, des systèmes de stockage et des équipements de télécommunications. Comme son nom l'indique, il sert surtout à stocker les informations nécessaires aux activités d'une entreprise. Par exemple, une banque peut recourir à un tel centre, lui confiant les informations relatives à ses clients. En pratique, presque toutes les entreprises de taille moyenne utilisent un tel centre. Quant aux grandes entreprises, elles en utilisent souvent plusieurs.
Les bases de données étant souvent cruciales au fonctionnement des entreprises, celles-ci sont très sensibles à leur protection. Pour cette raison, ces centres maintiennent de hauts niveaux de sécurité et de service dans le but d'assurer l'intégrité et le fonctionnement des appareils sur place.
Avant la bulle Internet, des millions de mètres carrés destinés à abriter de tels centres furent construits dans l'espoir de les voir occupés par des serveurs. Depuis, la concentration des centres s'est poursuivie, avec le développement de centres spécialisés pour lesquels les défis les plus importants sont la maîtrise de la climatisation et surtout de la consommation électrique. Ce mouvement a été intégré dans le green computing et vise à aboutir à des centres de traitement de données dits écologiques pour lesquels sont apparus des outils spécialisés.

Composantes
Pour permettre les missions principales du centre, elles doivent assurer la bonne connexion réseau (internet, intranet, etc.) et une haute disponibilité du système d'information. Pour cela, différentes applications logicielles gèrent les tâches essentielles de l'activité métier des clients. Parmi ces applications, on retrouve des gestionnaires de bases de données, des serveurs de fichiers et des serveurs d'applications.

Composantes physiques
Centre de données modulaire mobile en conteneur. Climatisation (précise et stable); Contrôle de la poussière (filtration de l'air); Unité de distribution de l'énergie; Bloc d'alimentation d'urgence, et une unité de secours (Générateur, UPS); Système perfectionné d'alerte d'incendie; Extinction automatique des incendies (par micro-gouttelettes ou gaz inerte); Plancher surélevé; Conduites pour câbles au-dessus et au-dessous du plancher; Surveillance par caméras en circuit fermé; Contrôle des accès, ainsi que sécurité physique; Surveillance 24-7 des serveurs dédiés (ordinateurs); Service de sécurité continuellement présent; Câbles de paires torsadées de cuivre en Ethernet (Fast ou Gigabit) pour liaisons inter-(jarretières-switches-routeurs-firewall); Fibres optiques pour liaisons inter-sites ou inter-(jarretières-switches-routeurs-firewall).
Un centre de traitement de données peut occuper une pièce, un étage ou un grand immeuble. On y retrouve des serveurs 1U (surnommés "boîtes à pizza") ou plus, "U" correspondant à une unité de hauteur de 4,445 cm (soit 1,75 pouce) empilés dans des baies, lesquelles sont arrangés pour former des rangées simples, ce qui permet de circuler facilement parmi les serveurs, tant à l'avant qu'à l'arrière. Quelques appareils, ordinateurs centraux par exemple, sont de dimensions semblables à ces baies. Ils sont souvent placés à leurs côtés.

Climatisation
La climatisation donne une température d'environ 20 degrés Celsius. Ce maintien est essentiel, puisque les appareils électroniques génèrent beaucoup de chaleur et deviennent défectueux lorsque la température s'élève au-delà d'une certaine limite.
Elle est également une composante fondamentale du centre de traitement, car s'il est mal conçu, il peut multiplier la consommation électrique nécessaire, pour garder une température stable des équipements et donc le coût mensuel de l'hébergement.
La climatisation peut être testée lors de la réception du centre de donnés. Pour cela, on utilise couramment des aérothermes qui permettent de créer un courant d'air chaud. On peut aussi utiliser des bancs de charge rackables qui peuvent être insérés dans les baies si celles-ci sont présentes.
Principe du refroidissement des baies.
Organisation de la salle pour la climatisation: L'air chaud est aspiré et de l'air froid insufflé, sans organisation particulière, c'est le minimum indispensable, mais également le moins optimal, car l'air chaud et l'air froid se mélangent, demandant une plus grande consommation électrique pour la climatisation.
Il est généralement conseillé de mettre des caches dans les baies, aux emplacement inutilisés, afin que le flux d'air froid vers chaud se concentre dans les équipements, en dehors du cas précité, où cela ne sert à rien. Organisation simple en couloir chaud et couloir froid, un couloir sur deux insuffle l'air froid, par le côté avant des serveurs, via des baies grillagées, l'autre couloir aspire l'air chaud par l'arrière. Il est à noter que certains équipements réseau, comme les commutateurs réseau, peuvent avoir le refroidissement avant arrière ou arrière avant, permettant d'organiser de façon plus pratique le câblage, selon les besoins. Le rendement est meilleur, mais l'air chaud peut encore partiellement se mélanger, à l'air froid en bout de rangée de baie ou par-dessus les baies; Utilisation d'un cube de confinement avec corridor chaud, ou froid, selon les choix des constructeurs. Le 'cube' dans lesquelles sont placées les baies, comporte un plafond et des portes à double ou triple vitrage, réduisant considérablement les échanges de température, seul un côté des baies échange l'air, avec les serveurs orientés en fonction du choix corridor chaud ou corridor froid. C'est aujourd'hui la solution optimale.

Systèmes de production de l'air froid
datadock au port autonome de Strasbourg est refroidi à l'eau.
Le compresseur frigorifique est la base des systèmes de refroidissement, cependant, ils peuvent comporter des bains liquides permettant d'améliorer le rendement. Le free cooling (refroidissement à air) permet de limiter le recours à des refroidisseurs et ainsi de réduire la facture énergétique. Le free cooling n'est intéressant que dans les implantations où l'air extérieur est froid suffisamment longtemps durant l'année.; La climatisation peut être complétée par un refroidissement à eau, qui est 4 000 fois plus efficace que l'air pour conduire la chaleur.

Composantes "Réseau"
Ce sont notamment : les routeurs ;; les commutateurs ;; le pare-feu ;; les passerelles ;; le système de détection d'intrusion logicielle ;; etc.

Sécurité
Salle de calcul du CERN.
L'environnement physique des centres est sous stricte surveillance : La surveillance du bon fonctionnement de la climatisation, elle-même essentielle au bon fonctionnement du matériel électronique.; L'alimentation de secours peut être fournie via un UPS et un générateur électrique ou via un groupe tournant (no-break) couplé à un accumulateur cinétique.; Dans le but de prévenir une perte d'alimentation électrique, toutes les composantes électriques, y compris les systèmes de secours, sont habituellement doublées. Les serveurs dits essentiels sont de plus alimentés par un système qui fait appel à deux sources électriques indépendantes à l'intérieur du centre.; Les centres ont habituellement un plancher surélevé de 60 cm, fait de dalles amovibles. Cet espace permet la libre circulation de l'air, tout comme il facilite le câblage d'alimentation et de données par des chemins de câble différents. Cependant, des centres de données sont sans plancher technique (alimentation par le dessus des racks, afin de supporter plus facilement des éléments lourds de type mainframe (IBM z10, etc.).
Bouteille d'extinction FM-200 dans un centre de calcul. Ils ont souvent des systèmes complexes de prévention et d'extinction des incendies. Les centres modernes sont souvent équipés de deux systèmes d'alarme. Le premier détecte les particules chaudes émises par les composantes surchauffées de l'équipement, particules qui provoquent souvent un feu. De cette façon, il est possible d'éliminer à sa source un foyer d'incendie (parfois, il suffit d'éteindre un ensemble à soudure pour éliminer le risque d'incendie). Un deuxième système sert à activer un ensemble d'activités si un début d'incendie se manifeste. Ces systèmes sont également dédiés à une portion du centre de traitement de données. Couplés à d'excellentes portes anti-feu et autres appareils de confinement, il est possible de contrôler le feu et de l'éteindre sans affecter le reste du bâtiment.; Les systèmes conventionnels d'extinction du feu sont aussi nocifs que le feu pour les composants électroniques, c'est pourquoi des procédés alternatifs ont été développés. Certains utilisent l'azote, l'argonite, le FM-200 ou le Novec(tm)1230 FK-5-1-12, alors que d'autres se rabattent sur l'émission de fines particules d'eau ultra-pure (cette eau n'est pas électriquement conductrice, ce qui n'endommage pas les composants électroniques).; La sécurité est aussi essentielle au fonctionnement de tels centres. L'accès physique à ces centres est restreinte au personnel autorisé, tout comme des caméras vidéo permettent de suivre les personnes sur place. Également, des gardes de sécurité veillent si le centre est grand ou contient des informations considérées comme essentielles.

L'ingénierie thermique des centres de données
En 2012, dans un Centre de traitement de données, les onduleurs et la climatisation consomment la moitié de l'énergie du centre. Les serveurs modernes (2012) peuvent supporter jusqu'à 45 C (à ne jamais dépasser), mais demandent une température de 20 à 35 C. Or, malgré les progrès faits sur les composants (plus efficients), les serveurs tendent à être de plus en plus compacts et denses (par surface). L'efficience énergétique du serveur et du centre de données sont en cours d'amélioration (optimisation de la conception à l'utilisation), adaptation du besoin à la puissance du serveur, free-cooling par eau ou air. La climatisation est souvent surdimensionnée mais néanmoins en partie inefficace, en raison d'une homogénéisation thermique du volume d'air, due à une mauvaise disposition des matériels dans les salles de centres de données. De nouveaux serveurs plus tolérants à la chaleur peuvent supporter de petits "pics" de chaleur et permettent de limiter ce surdimensionnement, si les équipements sont judicieusement placés. C'est le rôle (métier) du concepteur de centre de données de mieux concevoir (ou réaménager) les salles pour pouvoir gérer et optimiser la puissance informatique en situation confinée (via des allées froides cloisonnées ou allées chaudes cloisonnées ou confinées par exemple ; les allées chaudes sont celles qui sont en arrière des serveurs, et les allées froides sont celles qui sont devant). Idéalement, les rangées de serveurs ne doivent pas dépasser 20 C ou un peu plus pour les matériels récents. Cet urbanisme s'appuie sur la simulation aéraulique, pour améliorer la gestion (avec éventuelle récupération-valorisation) de la chaleur dégagée par les serveurs, et pour aménager l'intérieur du centre de données, avec le meilleur positionnement des serveurs dans de bonnes conditions de poids, de densité, d'aération, de longueur de câbles. Cette activité est située au croisement entre les règles de l'informatique et celles de la connexion électrique et du thermicien, tout en devant faciliter la maintenance.
Une bonne "urbanisation" peut fortement améliorer l'efficience énergétique du centre de données. De la fin des années 2000 à 2012, on est passé de 10 à 50 % de "serveurs virtuels" ; ces derniers peuvent aussi contribuer à optimiser la gestion des flux d'énergie.
Indésirable ici, les calories sont valorisable dans d'autres domaines: chauffage des logements, des bureaux ou même d'arboretum.

Réseau
Supervision du réseau dans une salle de contrôle d'un centre de données.
Les communications à l'intérieur d'un centre se font maintenant presque exclusivement par Internet Protocol. Il contient donc des routeurs, des commutateurs et tout autre équipement qui permet d'assurer la communication entre les serveurs et le monde extérieur. La redondance est parfois obtenue en faisant usage de multiples équipements réseau de marques différentes.
Quelques serveurs servent à fournir aux utilisateurs de la société les services Internet et Intranet de base dont ils ont besoin : courriel, proxy, DNS, fichiers, etc.
Des équipements de sécurité réseau y sont aussi présents : pare-feu, VPN, systèmes de détection d'intrusion, etc. ainsi que des systèmes de monitoring du réseau et de certaines applications.

Applications
Le but principal d'un centre de traitement de données est d'exécuter des applications qui traitent des données essentielles au fonctionnement d'une société. Ces applications peuvent être conçues et développées en interne par l'entreprise cliente ou par un fournisseur de progiciel de gestion d'entreprise. Il peut s'agir typiquement de ERP et CRM.
Souvent, ces applications sont réparties dans plusieurs ordinateurs, chacun exécutant une partie de la tâche. Les composantes les plus habituelles sont des systèmes de gestion de base de données, des serveurs de fichiers, des serveurs d'applications, des middleware.

Gestion de la capacité d'un Datacenter
Le cycle de vie de la capacité d'un Datacenter
La capacité d'utilisation d'un datacenter peut être limitée par plusieurs paramètres. Sur le long terme, les principales limites que rencontreront les exploitants seront la surface utilisable, puis la puissance disponible.
Dans la première phase de son cycle de vie un datacenter verra une croissance plus rapide de sa surface occupée que de l'énergie consommée.
Avec la densification constante des nouvelles technologies des matériels informatiques, le besoin en énergie va devenir prépondérant, équilibrant puis dépassant le besoin en superficie (deuxième puis troisième phase du cycle).
Le développement et la multiplicité des appareils connectés, des besoins en stockage et traitements des données font que les besoins datacenters croissent de plus en plus rapidement. Il est donc important de définir une stratégie datacenter avant d'être "dos au mur". Le cycle de décision, conception, construction est de plusieurs années. Il est donc important d'initier cette réflexion stratégique lorsque le datacenter atteint 50% de son énergie consommée.
Le maximum d'occupation d'un datacenter doit se stabiliser autour des 85% tant en énergie qu'en superficie occupée. En effet, les ressources ainsi ménagées serviront d'espace de manoeuvre pour gérer les remplacements de matériel et permettra ainsi la cohabitation temporaire des anciennes et nouvelles générations.
Si cette limite vient à être dépassée durablement, il ne devient plus possible de procéder au remplacement des matériels, ce qui conduit inexorablement vers l'étouffement du système d'information.
Le datacenter est une ressource à part entière du Système d'Information, avec ses propres contraintes de temps et de gestion (durée de vie 25 ans), il doit donc être pris en compte dans le cadre des plans à moyens termes du SI (entre 3 et 5 ans).

Localisation des centres de traitement de données
Centre de traitement de Google de The Dalles en Oregon.
En 2011, on dénombrait 2 087 centres de traitement de données dans le monde. Le Groupement des industries de l'équipement électrique, du contrôle-commande et des services associés (Gimélec) estime à 130 le nombre de centres de traitement de données d'offreurs en France dont 40 % en région parisienne. Globalement les centres de traitement sont dispersés sur l'ensemble du territoire avec des zones de concentration en partie liées au réseau urbain sur les départements de Paris, d'Île-de-France (essentiellement Hauts de Seine, Seine Saint Denis), le Nord et les Bouches du Rhône. L'ouest et la région Rhône-Alpes sont également des zones privilégiées.
Les services informatiques des grandes entreprises sont généralement implantés dans des centres de traitement de données, dédiés ou mutualisés. Les plus gros centres dans le monde sont ceux des géants de l'internet comme Google, qui utilise des infrastructures modulaires basées sur des conteneurs qui peuvent héberger jusqu'à 1 160 serveurs (voir Plateforme Google), ou Facebook qui a étendu son centre de traitement de Prineville dans l'Oregon. Amazon a lui aussi implanté son centre de traitement dans l'Oregon compte tenu du faible coût de l'énergie dans cet état. Apple a construit son plus gros centre à Maiden en Caroline du Nord, pour soutenir sa stratégie de développement de l'iCloud.

Caractéristiques des serveurs dans les centres de données
Les serveurs tendent à être virtualisés. En France, un tiers des serveurs hébergés dans des centres de données serait virtualisé et un cinquième à haute densité en 2011. Ces taux devaient doubler d'ici 2013.

Enjeux et impacts environnementaux
L'empreinte écologique globale des centres de données grandit rapidement, mais elle aurait pu être réduite par une optimisation et un partage des ressources (de 25 % environ en 2010) et elle peut encore l'être.
Les impacts environnementaux se concentrent lors de : 1) la fabrication : des bâtiments, des équipements liés aux bâtiments (groupes froid, groupes électrogènes, onduleurs, etc.) et des équipements informatiques et télécoms qu'ils contiennent ;; 2) l'utilisation du centre de données.
La fabrication concentre les pollutions et l'épuisement des stocks de ressources non renouvelables. L'utilisation se traduit essentiellement par des émissions de gaz à effet de serre (liées à la production de l'énergie consommée par le centre de données) et des émissions des déchets d'équipements électriques et électroniques (DEEE).
Des prospectivistes estiment que le volume des données numériques transportées par le Net pourrait être multiplié par 40 à 50 d'ici à 2020.
Deux facteurs pouvant être mieux maîtrisés et-ou valorisés sont :

Consommation d'électricité
Malgré des gains d'efficience énergétique des processeurs et en matière d'optimisation des réseaux et d'efficacité énergétique des matériels informatiques, en raison de l'explosion des besoins, les gros centres de traitement de données sont des systèmes physiques et cybernétiques (Cyber-Physical System) qui consomment des quantités importantes et croissantes d'électricité ; Selon un rapport Votre cloud est-il Net (avril 2012), "Certains centres de traitement des données consomment autant d'électricité que 250 000 foyers européens. Si le "cloud" était un pays, il se classerait (en 2012) au 5e rang mondial en termes de demande en électricité, et ses besoins devraient être multipliés par trois d'ici à 2020.". D'après Qarnot ComputingEn France, en 2013, plus de deux cents centres de données consomment plus de 7 % de l'électricité du pays. Un centre de données de 10 000 m2 consommerait autant qu'une ville de 50 000 habitants et "À l'échelle européenne, la Commission estimait en 2008 que les centres de données consommaient 56 TWh, dont la moitié pour refroidir les bâtiments".
La provenance de l'électricité influe sur le bilan écologique global. Au début du XXIe siècle, de grands opérateurs n'utilisent que l'énergie fossile (pétrole, gaz voire charbon) ou le nucléaire alors que d'autres investissent aussi dans les énergies renouvelables (éolien notamment) pour se fournir en électricité verte.

Production de chaleur
Les centres de traitement de données émettent beaucoup de chaleur. Ils doivent être continuellement réfrigérés ou tempérés. Les calories extraites des serveurs sont le plus souvent gaspillées (rejetées dans l'environnement où elles aggravent les effets du changement climatique en élevant la température de l'air, ou des eaux qui reçoivent les eaux de refroidissement).
Depuis peu, des expériences visent à récupérer les "joules" et à les réutiliser pour des besoins énergétiques locaux ou de proximité (chauffage, chauffage urbain, réseau de chaleur). Par exemple : l'université d'Amsterdam reçoit une eau gratuitement chauffée par le centre de données d'Equinix. À Roubaix (5 centres d'OVH, leader français de l'hébergement) sont refroidis par eaux et contribuent à chauffer des bâtiments proches. À Clichy, "Global Switch" chauffe une serre tropicale et aide les jardiniers à produire les fleurs de la ville. Le réseau Dalkia de chauffage urbain de Paris-Val d'Europe (Marne-la-vallée) récupère les calories d'un centre de données de la ZAC du Prieuré pour notamment chauffer un "centre aquatique intercommunal" et à terme offrir "26 GWh de chaleur pour chauffer 600 000 m2 de locaux tertiaires et d'équipements, répartis sur 150 hectares", soit "5 400 tonnes de CO2-an, (...) l'équivalent des rejets de 5 000 véhicules. En Suisse, IBM chauffe la piscine d'Uitikon."
Autre concept : le "siloctets" écoconcept de l'entreprise québécoise "Vert.com" est un silo de serveurs, grand comme un bâtiment de ville, passivement refroidi et dont les calories peuvent être récupérées par une centrale en toiture et distribuée aux voisins, testé à l'Éco-campus Hubert-Reeves dans le Technoparc de Montréal.

Bilan carbone
"Akamai qui gère un gros volume de trafic sur le Net, semble être la première entreprise de ce secteur à avoir calculé son bilan carbone, au moyen de l'"indicateur d'efficacité de l'utilisation carbone" (Carbon Usage Effectiveness) ; En 2012, Les autres entreprises n'avaient pas mis à profit cet indicateur". Deux leviers pour améliorer le bilan carbone sont les économies d'énergie d'une part, et l'écoconception et la valorisation des calories produites d'autre part. Des mesures compensatoires (compensation carbone) sont aussi envisagées ou utilisées par certains acteurs. En 2011, le centre GrenoblIX est devenu le premier centre de données écologique en France.

Alternative distribuée
Pour répondre à ces trois enjeux et après que l'expérience SETI@home ait montré l'intérêt du "calcul distribué" (en utilisant dans ce cas des ordinateurs domestiques connectés à l'Internet), certains opérateurs comme AMD ont envisagé de décentraliser leurs centres de traitement de données en un réseau distribué tel que proposé par d'unités (petits centres de traitement de données bénéficiant chacun d'une éolienne, maillés entre eux par des fibres optiques).
Des installateurs et propriétaires de centres de traitement de données et des producteurs d'énergie pourraient à l'avenir associer leurs investissements dans un smart grid, éventuellement intégré dans l'"Internet de l'énergie" que Jeremy Rifkin propose dans son concept de "troisième révolution industrielle".
Le cloud computing pourrait alors évoluer vers un modèle totalement décentralisé, nécessitant une "gestion dynamique du refroidissement" (refroidir là où il faut et quand il faut, et passivement tant que possible), ainsi qu'une conception différente de la sécurité des serveurs et de leurs données, de la gestion distribuée des données, de la gestion de l'énergie et de la capacité des réseaux de centres de traitement de données à s'autoadapter aux fluctuations des besoins, mais aussi de l'énergie disponible. Leurs réponses doivent être plus élastiques(34), sans augmentation globale des consommations d'énergie, dans le cadre d'un green cloud qui reste à inventer.
Au début des années 2000, une solution complémentaire des précédentes apparait, qui pourraient être réservée aux données à fortement sécuriser. C'est de développer des réseaux de serveurs en grande partie virtuel (ou plus précisément partagés et distribués, utilisant une partie des ressources des ordinateurs familiaux et d'entreprises ou les utilisant quand leur propriétaire ne les utilisent pas ou les sous-utilisent, ce qui demande aussi de repenser la sécurité informatique). Pour cela, des systèmes d'allocation sécurisée des ressources et de répartition des tâches (éventuellement différées quand elles ne sont pas urgentes) doivent encore être testés et validés à grande échelle. Dans tous les cas la recherche et développement est à développer. La start-up française Qarnot Computing voir propose de délocaliser les serveurs chez les particuliers en lieu et place de radiateurs. Un premier essai concernera fin 2013 une centaine de logements HLM parisiens (RIVP dans le 15e arrondissement), qui seront gratuitement chauffés par de petits serveurs alimentés par le réseau électrique et connectés à l'Internet et via une plateforme sécurisée, dite "Q.Ware". Plus les processeurs travaillent, plus le chauffage est important, mais le résident du logement contrôle la température grâce à un thermostat. En été des serveurs classiques prennent le relais.

Classification des centres de traitement de données
L'organisme Uptime Institute a défini une certification internationalement reconnue des centres de traitement de données en quatre catégories, appelées "Tier": Tier I : Infrastructure non redondante, une seule alimentation électrique, climatisation non redondante.; Tier II : Les éléments de production de froid ou d'électricité sont redondants, mais la distribution d'électricité et de froid n'est pas redondante.; Tier III : Tous les composants sont maintenables sans arrêt de l'informatique.; Tier IV : Tolérance aux pannes. Aucune panne n'arrête l'informatique (réponse automatique). Absence de SPOF (Single Point of Failure).
À noter que le niveau "Tier III+" n'est pas officiel, même si l'usage commercial est parfois rencontré.
Seul l'Uptime Institute peut délivrer une certification. Aucun autre organisme n'est habilité à se prononcer sur le niveau de Tiering d'un centre informatique, encore moins par auto-évaluation.

Voir aussi

Articles connexes: Ordinateur central; Serveur informatique; Serveur lame; Client-serveur; Centre de traitement de données modulaire; Grappe de serveurs; Hyper-convergence; Open Compute Project.