Le modèle probabiliste de pertinence est une méthode probabiliste de représentation du contenu d'un document, proposée en 1976 par Robertson et Jones. Elle est utilisée en recherche d'information pour exprimer une estimation de la probabilité de pertinence d'un document par rapport à une requête, et ainsi classer une liste de documents dans l'ordre décroissant d'utilité probable pour l'utilisateur. L'une des applications directes de ce modèle est la méthode de pondération Okapi BM25, considérée comme l'une des plus performantes dans le domaine.

Modélisation
Étant donné une requête q, il s'agit d'estimer un score s(D) pour chaque document D de la base de données considérée. Ce score doit exprimer la probabilité relative que le document soit pertinent pour la requête considérée. Dans ce modèle, on s'intéresse en effet plus à l'ordre relatif des documents renvoyés qu'à leur pertinence absolue.
Similairement à d'autres modèles, on suppose que : il existe des documents pertinents pour cette requête du point de vue de l'utilisateur (ensemble R (-displaystyle R) de documents, les documents non pertinents étant le complément R (-displaystyle (-bar (R))) de cet ensemble dans la base); la pertinence d'un document est indépendante des jugements portés sur les autres documents; l'utilité d'un document pertinent est indépendante du nombre de documents pertinents précédemment renvoyé.
Sous ces conditions, on modélise la pertinence d'un document comme le ratio de probabilité que le document soit pertinent sur celle qu'il ne le soit pas : s (D q) - P (R D) P (R D) (-displaystyle s(Dq)-(-frac (P(RD))(P((-bar (R))D)))).
Considérant un vocabulaire T - t 1 ,.. t m (-displaystyle T-(t-(1),-dots ,t-(m))), un document est caractérisé par la présence (noté abusivement t i - 1 (-displaystyle t-(i)-1)) ou l'absence (t i - 0 (-displaystyle t-(i)-0)) de chaque terme dans son contenu. En utilisant notamment le théorème de Bayes on peut montrer que le score du modèle probabiliste peut se mettre sous la forme: s (D q) - i - 1 m w i t i (-displaystyle s(Dq)--sum -(i-1)(m)(w-(i)-times t-(i))).
Où le poids w i (-displaystyle w-(i)) dépend de la probabilité de présence du terme t i (-displaystyle t-(i)) dans l'ensemble des documents pertinent et son complément.

Expression du poids
Considérons une base de N (-displaystyle N) documents, dont n (-displaystyle n) sont considérés pertinents pour la requête. En notant R i (-displaystyle R-(i)) le nombre de documents contenant le terme t i (-displaystyle t-(i)), et r i (-displaystyle r-(i)) le nombre de documents pertinents parmi ceux-ci, le poids du modèle probabiliste est donné par : w i - l o g (r i n r i R i r i N R i n + r i) - l o g (r i (N R i n + r i) (n r i) (R i r i)) (-displaystyle w-(i)-log-left((-frac (-frac (r-(i))(n-r-(i)))(-frac (R-(i)-r-(i))(N-R-(i)-n+r-(i))))-right)-log-left((-frac (r-(i)(N-R-(i)-n+r-(i)))((n-r-(i))(R-(i)-r-(i))))-right)).
Pour éviter les poids aberrants (prosaïquement, les divisions par 0), on propose un lissage de la formule : w i - l o g (r i + 0.5 n r i + 0.5 R i r i + 0.5 N R i n + r i + 0.5) - l o g ((r i + 0.5) (N R i n + r i + 0.5) (n r i + 0.5) (R i r i + 0.5)) (-displaystyle w-(i)-log-left((-frac (-frac (r-(i)+0.5)(n-r-(i)+0.5))(-frac (R-(i)-r-(i)+0.5)(N-R-(i)-n+r-(i)+0.5)))-right)-log-left((-frac ((r-(i)+0.5)(N-R-(i)-n+r-(i)+0.5))((n-r-(i)+0.5)(R-(i)-r-(i)+0.5)))-right)).
Si on néglige de considérer les documents pertinents pour la requête (n - r i - 0 (-displaystyle n-r-(i)-0)), on retrouve l'expression dite probabiliste de la fréquence inverse de document : w i - l o g (N R i R i) (-displaystyle w-(i)-log-left((-frac (N-R-(i))(R-(i)))-right)).

Voir aussi: Modèle booléen; Modèle vectoriel; Lemme (linguistique); Moteur de recherche; Système de recherche d'information.